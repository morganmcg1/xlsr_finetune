{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data \n",
    "\n",
    "> Functions to process your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#all_slow\n",
    "import librosa\n",
    "import torchaudio\n",
    "from datasets import load_dataset, load_metric, concatenate_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up missing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def file_exists(e):\n",
    "    e['file_exists'] = os.path.isfile(e['path'])\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ds_file_exists(ds):\n",
    "    l_ds = len(ds)\n",
    "    ds = ds.map(file_exists)\n",
    "    l_not_found = l_ds-sum(ds['file_exists'])\n",
    "    if l_not_found == 0: print('All files found')\n",
    "    else: print(f\"{l_not_found} ({(l_not_found/l_ds)*100}%) files not found\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def filter_for_exists(ds, drop_exist_col=True):\n",
    "    # Filter dataset to only files that exist\n",
    "    ds = ds.filter(lambda example: example['file_exists'])\n",
    "\n",
    "    if drop_exist_col:\n",
    "        # drop file_exists column\n",
    "        ds = ds.remove_columns('file_exists')\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def drop_missing_files(ds, drop_exist_col=True):\n",
    "    ds = ds_file_exists(ds)\n",
    "    ds = filter_for_exists(ds, drop_exist_col)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset common_voice (data/common_voice/ga-IE/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f)\n",
      "Loading cached processed dataset at data/common_voice/ga-IE/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-3443a4ebc22019f7.arrow\n",
      "Loading cached processed dataset at data/common_voice/ga-IE/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-f013f7ba1baffc62.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found\n"
     ]
    }
   ],
   "source": [
    "test_ds = load_dataset(\"common_voice\", \"ga-IE\", split=\"test\", cache_dir='data')\n",
    "test_ds = test_ds.select([0,1,2,3,4,5,6,7,8,9])\n",
    "ds = drop_missing_files(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def add_ds(e, new_ds):\n",
    "    for f in e.keys():\n",
    "        e[f] = e[f] + new_ds[f]\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge two Datasets, note that they need to have the same columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def merge_ds(ds, new_ds, shuffle=True): \n",
    "    add_ds_func = partial(add_ds, new_ds=new_ds)\n",
    "    ds = ds.map(add_ds_func, batched=True, batch_size=-1, keep_in_memory=True)\n",
    "    if shuffle: ds = ds.shuffle()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56350214c954b62821cea61089f67e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds = merge_ds(test_ds, test_ds)\n",
    "assert len(ds) == 2*len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove special characters. This can be dataset and language specific, be careful about removing characters that may change the meaning of a word or sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�\\(\\)\\-\\*]'\n",
    "\n",
    "def remove_special_characters(batch, evaluate=False):\n",
    "    if evaluate: batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', \n",
    "                                            batch[\"sentence\"]).lower()\n",
    "    else: batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', \n",
    "                                            batch[\"sentence\"]).lower() + \" \"\n",
    "        \n",
    "    batch[\"sentence\"] = re.sub('[\\’]', '\\'', batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub('[\\’]', '\\'', batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub('[\\–]', '-', batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub('[\\—]', '-', batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub('[&]', ' and ', batch[\"sentence\"])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45645416d9a946c185c43061ad19f0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "wer = load_metric(\"wer\")\n",
    "\n",
    "import torchaudio\n",
    "resampler = torchaudio.transforms.Resample(48_000, 16_000)\n",
    "\n",
    "import re\n",
    "def speech_file_to_array_fn(batch):\n",
    "    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower()\n",
    "    batch[\"sentence\"] = re.sub('[\\’]', '\\'', batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub('[\\’]', '\\'', batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub('[\\–]', '-', batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub('[\\—]', '-', batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub('[&]', ' and ', batch[\"sentence\"])\n",
    "    \n",
    "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n",
    "    return batch\n",
    "\n",
    "test_ds = test_ds.map(speech_file_to_array_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495b1511e806476687887f67d6a784d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab\n",
    "Build a vocabulary from the entire set of letters in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def extract_all_chars(batch):\n",
    "    '''merge all texts into one and create set'''\n",
    "    all_text = \" \".join(batch[\"sentence\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_char_vocab(train_ds, test_ds=None):\n",
    "    train_vocab = train_ds.map(extract_all_chars, batched=True, batch_size=-1, \n",
    "                   keep_in_memory=True, remove_columns=train_ds.column_names)\n",
    "    if test_ds is not None: \n",
    "        test_vocab = test_ds.map(extract_all_chars, batched=True, batch_size=-1, \n",
    "                   keep_in_memory=True, remove_columns=test_ds.column_names)\n",
    "        vocab_list = list(set(train_vocab[\"vocab\"][0]) | set(test_vocab[\"vocab\"][0]))\n",
    "    else:  \n",
    "        vocab_list = list(set(train_vocab[\"vocab\"][0]))\n",
    "\n",
    "    vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8203469aff40508058310b01601093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = get_char_vocab(ds)\n",
    "assert len(vocab.keys()) == 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def process_vocab(vocab_dict):\n",
    "    vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "    del vocab_dict[\" \"]\n",
    "\n",
    "    vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "    vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = process_vocab(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract a processed, character level vocab from a Dataset and optioally save it as a json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def extract_vocab(train_ds, test_ds=None, save=True, save_dir='data', fn='vocab.json'):\n",
    "    vocab = get_char_vocab(train_ds, test_ds)\n",
    "    vocab = process_vocab(vocab)\n",
    "    if save:\n",
    "        Path(f\"{save_dir}\").mkdir(parents=True, exist_ok=True)\n",
    "        with open(f'{save_dir}/{fn}', 'w') as vocab_file:\n",
    "            json.dump(vocab, vocab_file)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba26826ce074afb9a4f7e4066062d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = extract_vocab(ds, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Audio Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the audio array and sampling rate from the file and resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def speech_file_to_array(batch, resample=True, new_sr=16_000, evaluate=False):\n",
    "    try:\n",
    "        speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "        \n",
    "        if resample: \n",
    "            if evaluate:\n",
    "                resampler = torchaudio.transforms.Resample(sampling_rate, new_sr)\n",
    "                batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n",
    "            else:\n",
    "                batch[\"speech\"] = librosa.resample(np.asarray(speech_array[0].numpy()), sampling_rate, new_sr)\n",
    "            batch[\"sampling_rate\"] = new_sr\n",
    "        else: \n",
    "            batch[\"speech\"] = speech_array[0].numpy()\n",
    "            batch[\"sampling_rate\"] = sampling_rate\n",
    "    except:\n",
    "        batch[\"speech\"] = np.array([0])\n",
    "        batch[\"sampling_rate\"] = 0\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e68fd8299de4a8eada721f61b7d2906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sp2a = partial(speech_file_to_array, new_sr=8_000)\n",
    "ds =  ds.map(sp2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_data.ipynb.\n",
      "Converted 02_aug.ipynb.\n",
      "Converted 03_training.ipynb.\n",
      "Converted 04_evaluation.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "## hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
