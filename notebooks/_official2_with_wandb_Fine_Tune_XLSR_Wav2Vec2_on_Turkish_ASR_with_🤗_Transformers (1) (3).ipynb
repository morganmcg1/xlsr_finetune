{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Fine-tuning XLSR-Wav2Vec2 for Multi-Lingual ASR with 🤗 Transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wav2Vec2 is a pretrained model for Automatic Speech Recognition (ASR) and was released in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/) by Alexei Baevski, Michael Auli, and Alex Conneau.  Soon after the superior performance of Wav2Vec2 was demonstrated on the English ASR dataset LibriSpeech, *Facebook AI* presented XLSR-Wav2Vec2 (click [here](https://arxiv.org/abs/2006.13979)). XLSR stands for *cross-lingual  speech representations* and refers to XLSR-Wav2Vec2`s ability to learn speech representations that are useful across multiple languages.\n",
    "\n",
    "Similar to Wav2Vec2, XLSR-Wav2Vec2 learns powerful speech representations from hundreds of thousands of hours of speech in more than 50 languages of unlabeled speech. Similar, to [BERT's masked language modeling](http://jalammar.github.io/illustrated-bert/), the model learns contextualized speech representations by randomly masking feature vectors before passing them to a transformer network.\n",
    "\n",
    "![wav2vec2_structure](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/xlsr_wav2vec2.png)\n",
    "\n",
    "The authors show for the first time that massively pretraining an ASR model on cross-lingual unlabeled speech data, followed by language-specific fine-tuning on very little labeled data achieves state-of-the-art results. See Table 1-5 of the official [paper](https://arxiv.org/pdf/2006.13979.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will give an in-detail explanation of how XLSR-Wav2Vec2's pretrained checkpoint can be fine-tuned on a low-resource ASR dataset of any language. Note that in this notebook, we will fine-tune XLSR-Wav2Vec2 without making use of a language model. It is much simpler and more efficient to use XLSR-Wav2Vec2 without a language model, but better results can be achieved by including a language model. \n",
    "\n",
    "For demonstration purposes, we fine-tune the [wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on the low resource Turkish ASR dataset of [Common Voice](https://huggingface.co/datasets/common_voice) that contains just ~6h of validated training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XLSR-Wav2Vec2 is fine-tuned using Connectionist Temporal Classification (CTC), which is an algorithm that is used to train neural networks for sequence-to-sequence problems and mainly in Automatic Speech Recognition and handwriting recognition. \n",
    "\n",
    "I highly recommend reading the blog post [Sequence Modeling with CTC (2017)](https://distill.pub/2017/ctc/) very well-written blog post by Awni Hannun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let's install both `datasets` and `transformers` from master. Also, we need the `torchaudio` and `librosa` package to load audio files and the `jiwer` to evaluate our fine-tuned model using the [word error rate (WER)](https://huggingface.co/metrics/wer) metric ${}^1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets==1.4.1\n",
    "# !pip install transformers==4.4.0\n",
    "!pip install torchaudio\n",
    "!pip install librosa\n",
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install wandb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_ENTITY=wandb\n",
      "env: WANDB_PROJECT=xlsr-irish\n",
      "env: WANDB_LOG_MODEL=true\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "# W&B company account\n",
    "%env WANDB_ENTITY = wandb\n",
    "entity = os.environ[\"WANDB_ENTITY\"]\n",
    "\n",
    "# Choose the public W&B project\n",
    "%env WANDB_PROJECT = xlsr-irish\n",
    "project_name = os.environ[\"WANDB_PROJECT\"]\n",
    "\n",
    "# Log your trained model to W&B as an Artifact\n",
    "%env WANDB_LOG_MODEL = true "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "${}^1$ In the [paper](https://arxiv.org/pdf/2006.13979.pdf), the model was evaluated using the phoneme error rate (PER), but by far the most common metric in ASR is the word error rate (WER). To keep this notebook as general as possible we decided to evaluate the model using WER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmorgan-test\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data, Tokenizer, Feature Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASR models transcribe speech to text, which means that we both need a feature extractor that processes the speech signal to the model's input format, *e.g.* a feature vector, and a tokenizer that processes the model's output format to text. \n",
    "\n",
    "In 🤗 Transformers, the XLSR-Wav2Vec2 model is thus accompanied by both a tokenizer, called [Wav2Vec2CTCTokenizer](https://huggingface.co/transformers/master/model_doc/wav2vec2.html#wav2vec2ctctokenizer), and a feature extractor, called [Wav2Vec2FeatureExtractor](https://huggingface.co/transformers/master/model_doc/wav2vec2.html#wav2vec2featureextractor).\n",
    "\n",
    "Let's start by creating the tokenizer responsible for decoding the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Wav2Vec2CTCTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [pretrained Wav2Vec2 checkpoint]( ) maps the speech signal to a sequence of context representations as illustrated in the figure above. A fine-tuned XLSR-Wav2Vec2 checkpoint needs to map this sequence of context representations to its corresponding transcription so that a linear layer has to be added on top of the transformer block (shown in yellow). This linear layer is used to classifies each context representation to a token class analogous how, *e.g.*, after pretraining a linear layer is added on top of BERT's embeddings for further classification - *cf.* with *\"BERT\"* section of this [blog post](https://huggingface.co/blog/warm-starting-encoder-decoder).\n",
    "\n",
    "The output size of this layer corresponds to the number of tokens in the vocabulary, which does **not** depend on XLSR-Wav2Vec2's pretraining task, but only on the labeled dataset used for fine-tuning. So in the first step, we will take a look at Common Voice and define a vocabulary based on the dataset's transcriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's go to [Common Voice](https://commonvoice.mozilla.org/en/datasets) and pick a language to fine-tune XLSR-Wav2Vec2 on. For this notebook, we will use Turkish. \n",
    "\n",
    "For each language-specific dataset, you can find a language code corresponding to your chosen language. On [Common Voice](https://commonvoice.mozilla.org/en/datasets), look for the field \"Version\". The language code then corresponds to the prefix before the underscore. For Turkish, *e.g.* the language code is `\"tr\"`.\n",
    "\n",
    "Great, now we can use 🤗 Datasets' simple API to download the data. The dataset name will be `\"common_voice\"`, the config name corresponds to the language code - `\"tr\"` in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Voice has many different splits including `invalidated`, which refers to data that was not rated as \"clean enough\" to be considered useful. In this notebook, we will only make use of the splits `\"train\"`, `\"validation\"` and `\"test\"`. \n",
    "\n",
    "Because the Turkish dataset is so small, we will merge both the validation and training data into a training dataset and simply use the test data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset common_voice (data/common_voice/ga-IE/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f)\n",
      "Reusing dataset common_voice (data/common_voice/ga-IE/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f)\n"
     ]
    }
   ],
   "source": [
    "# common_voice_train = load_dataset(\"common_voice\", \"tr\", split=\"train+validation\", cache_dir='data')\n",
    "# common_voice_test = load_dataset(\"common_voice\", \"tr\", split=\"test\", cache_dir='data')\n",
    "\n",
    "common_voice_train = load_dataset(\"common_voice\", \"ga-IE\", split=\"train+validation\", cache_dir='data')\n",
    "common_voice_test = load_dataset(\"common_voice\", \"ga-IE\", split=\"test\", cache_dir='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['client_id', 'path', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
       "     num_rows: 1038\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['client_id', 'path', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
       "     num_rows: 506\n",
       " }))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_train, common_voice_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Some English Data + Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset common_voice (data/common_voice/en/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f)\n"
     ]
    }
   ],
   "source": [
    "common_voice_en = load_dataset(\"common_voice\", \"en\", split='train[:1%]',  cache_dir='data' )  \n",
    "                              # ,download_mode='force_redownload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['client_id', 'path', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
       "    num_rows: 5643\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out en file paths with missing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at data/common_voice/en/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-ebbb341e9ecbe98b.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8149920255183413\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def file_exists(e):\n",
    "    e['file_exists'] = os.path.isfile(e['path'])\n",
    "    return e\n",
    "\n",
    "common_voice_en = common_voice_en.map(file_exists)\n",
    "\n",
    "print(sum(common_voice_en['file_exists'])/len(common_voice_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at data/common_voice/en/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-253db73968b7acc3.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5643\n",
      "4599\n"
     ]
    }
   ],
   "source": [
    "# start_with_ar = dataset.filter(lambda example: example['sentence1'].startswith('Ar'))\n",
    "print(len(common_voice_en))\n",
    "common_voice_en_filtered = common_voice_en.filter(lambda example: example['file_exists'])\n",
    "print(len(common_voice_en_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the \"file_exists\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['accent', 'age', 'client_id', 'down_votes', 'gender', 'locale', 'path', 'segment', 'sentence', 'up_votes'],\n",
       "    num_rows: 4599\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_en_filtered = common_voice_en_filtered.remove_columns('file_exists')\n",
    "common_voice_en_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge English and Irish datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839a69db0d2a4a67ada78930408fd03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['accent', 'age', 'client_id', 'down_votes', 'gender', 'locale', 'path', 'segment', 'sentence', 'up_votes'],\n",
       "    num_rows: 5637\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_en_data(e):\n",
    "    for f in e.keys():\n",
    "        e[f] = e[f] + common_voice_en_filtered[f]\n",
    "    return e\n",
    "\n",
    "common_voice_train = common_voice_train.map(add_en_data, batched=True, batch_size=-1, keep_in_memory=True)\n",
    "\n",
    "common_voice_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many ASR datasets only provide the target text, `'sentence'` for each audio file `'path'`. Common Voice actually provides much more information about each audio file, such as the `'accent'`, etc. However, we want to keep the notebook as general as possible, so that we will only consider the transcribed text for fine-tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
    "common_voice_test = common_voice_test.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a short function to display some random samples of the dataset and run it a couple of times to get a feeling for the transcriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The parish was part of the North Curry Hundred.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Political friends included Richard M. Nixon, Ronald Reagan and Gerald R. Ford.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Since then, they have moved back to Blue Valley.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An balla ard é sin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ellis also admitted sharing exit-poll data with his cousins by phone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>With the collapse of the guerrilla movement, Yavelberg and Lamarca fled to Bahia.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>An electrician knows how to solder.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Very low humidity can create discomfort, respiratory problems, and aggravate allergies in some individuals.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>These universities are strongly focused on fields like computer science, engineering or business schools.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Lords were literally the peers of the realm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>An Ciste Caomhnaithe Bradán.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>He was at one time designated the honorary mayor of West Los Angeles.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Did not you understand that?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"In Blake's words, They don't hand these out like candy.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>His major sponsor is Algario Communications, a Toronto-based communications and sales development company.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>He kidnaps her, holds her captive, rapes her, and cuts out her tongue.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>So, no one can find them!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>He has a younger sister called Shita-Teru-Hime.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Styles highlighted are sculpture in bronze and stone, watercolors, painting, and framing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>agus comhghairdeas libh arís</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(common_voice_train.remove_columns([\"path\"]), num_examples=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! The transcriptions look fairly clean. Having translated the transcribed sentences (I'm sadly not a native speaker in Turkish), it seems that the language corresponds more to written text than noisy dialogue. This makes sense taking into account that [Common Voice](https://huggingface.co/datasets/common_voice) is a crowd-sourced read speech corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the transcriptions contain some special characters, such as `,.?!;:`. Without a language model, it is much harder to classify speech chunks to such special characters because they don't really correspond to a characteristic sound unit. *E.g.*, the letter `\"s\"` has a more or less clear sound, whereas the special character `\".\"` does not.\n",
    "Also in order to understand the meaning of a speech signal, it is usually not necessary to include special characters in the transcription.\n",
    "\n",
    "In addition, we normalize the text to only have lower case letters and append a word separator token at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "additional chars to remove = ( ) -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�\\(\\)\\-\\*]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower() + \" \"\n",
    "    batch[\"sentence\"] = re.sub('[\\’]', '\\'', batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub('[\\’]', '\\'', batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub('[\\–]', '-', batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub('[\\—]', '-', batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub('[&]', ' and ', batch[\"sentence\"])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21cb66cd5a8a4a439d88a48f121f597a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5637.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at data/common_voice/ga-IE/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-699a48674f45b3f2.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "common_voice_train = common_voice_train.map(remove_special_characters)\n",
    "common_voice_test = common_voice_test.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>often focusing screens are available in variants with different etched markings for various purposes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>óráid an uachtaráin ar ócáid do phardún mhaolra seoighe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spatial statistics typically result primarily from observation rather than experimentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the barrier was called the luminous veil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this becomes prohibitive for realistically large values of n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>many new julfan armenians later settled in manila hong kong and also in australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tá sí ag imirt i lár na páirce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>she had two children from her marriage to burgess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the source of the name brumley is also disputed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>he was my true son</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(common_voice_train.remove_columns([\"path\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! This looks better. We have removed most special characters from transcriptions and normalized them to lower-case only.\n",
    "\n",
    "In CTC, it is common to classify speech chunks into letters, so we will do the same here. \n",
    "Let's extract all distinct letters of the training and test data and build our vocabulary from this set of letters.\n",
    "\n",
    "We write a mapping function that concatenates all transcriptions into one long transcription and then transforms the string into a set of chars. \n",
    "It is important to pass the argument `batched=True` to the `map(...)` function so that the mapping function has access to all transcriptions at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chars(batch):\n",
    "  all_text = \" \".join(batch[\"sentence\"])\n",
    "  vocab = list(set(all_text))\n",
    "  return {\"vocab\": [vocab], \"all_text\": [all_text]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb45544b67e047d882d75865d6edace8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68c6c36c671488eb21e646be1d83988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_train = common_voice_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_train.column_names)\n",
    "vocab_test = common_voice_test.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_test.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create the union of all distinct letters in the training dataset and test dataset and convert the resulting list into an enumerated dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_test[\"vocab\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34,\n",
       " {'k': 0,\n",
       "  'ó': 1,\n",
       "  'a': 2,\n",
       "  'é': 3,\n",
       "  'c': 4,\n",
       "  'h': 5,\n",
       "  'z': 6,\n",
       "  'q': 7,\n",
       "  'd': 8,\n",
       "  's': 9,\n",
       "  'i': 10,\n",
       "  'y': 11,\n",
       "  't': 12,\n",
       "  'j': 13,\n",
       "  'x': 14,\n",
       "  ' ': 15,\n",
       "  'n': 16,\n",
       "  'm': 17,\n",
       "  'b': 18,\n",
       "  'ú': 19,\n",
       "  'á': 20,\n",
       "  'e': 21,\n",
       "  'r': 22,\n",
       "  'g': 23,\n",
       "  'l': 24,\n",
       "  '-': 25,\n",
       "  'í': 26,\n",
       "  'f': 27,\n",
       "  \"'\": 28,\n",
       "  'w': 29,\n",
       "  'o': 30,\n",
       "  'u': 31,\n",
       "  'v': 32,\n",
       "  'p': 33})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "len(vocab_dict.keys()), vocab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, we see that all letters of the alphabet occur in the dataset (which is not really surprising) and we also extracted the special characters `\" \"` and `'`. Note that we did not exclude those special characters because: \n",
    "\n",
    "- The model has to learn to predict when a word is finished or else the model prediction would always be a sequence of chars which would make it impossible to separate words from each other.\n",
    "- From the transcriptions above it seems that words that include an apostrophe, such as `maktouf'un` do exist in Turkish, so I decided to keep the apostrophe in the dataset. This might be a wrong assumption though.\n",
    "\n",
    "One should always keep in mind that the data-preprocessing is a very important step before training your model. E.g., we don't want our model to differentiate between `a` and `A` just because we forgot to normalize the data. The difference between `a` and `A` does not depend on the \"sound\" of the letter at all, but more on grammatical rules - *e.g.* use a capitalized letter at the beginning of the sentence. So it is sensible to remove the difference between capitalized and non-capitalized letters so that the model has an easier time learning to transcribe speech. \n",
    "\n",
    "It is always advantageous to get help from a native speaker of the language you would like to transcribe to verify whether the assumptions you made are sensible, *e.g.* I should have made sure that keeping `'`, but removing other special characters is a sensible choice for Turkish. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it clearer that `\" \"` has its own token class, we give it a more visible character `|`. In addition, we also add an \"unknown\" token so that the model can later deal with characters not encountered in Common Voice's training set. \n",
    "\n",
    "Finally, we also add a padding token that corresponds to CTC's \"*blank token*\". The \"blank token\" is a core component of the CTC algorithm. For more information, please take a look at the \"Alignment\" section [here](https://distill.pub/2017/ctc/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': 0,\n",
       " 'ó': 1,\n",
       " 'a': 2,\n",
       " 'é': 3,\n",
       " 'c': 4,\n",
       " 'h': 5,\n",
       " 'z': 6,\n",
       " 'q': 7,\n",
       " 'd': 8,\n",
       " 's': 9,\n",
       " 'i': 10,\n",
       " 'y': 11,\n",
       " 't': 12,\n",
       " 'j': 13,\n",
       " 'x': 14,\n",
       " 'n': 16,\n",
       " 'm': 17,\n",
       " 'b': 18,\n",
       " 'ú': 19,\n",
       " 'á': 20,\n",
       " 'e': 21,\n",
       " 'r': 22,\n",
       " 'g': 23,\n",
       " 'l': 24,\n",
       " '-': 25,\n",
       " 'í': 26,\n",
       " 'f': 27,\n",
       " \"'\": 28,\n",
       " 'w': 29,\n",
       " 'o': 30,\n",
       " 'u': 31,\n",
       " 'v': 32,\n",
       " 'p': 33,\n",
       " '|': 15,\n",
       " '[UNK]': 34,\n",
       " '[PAD]': 35}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, now our vocabulary is complete and consists of 39 tokens, which means that the linear layer that we will add on top of the pretrained XLSR-Wav2Vec2 checkpoint will have an output dimension of 39."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now save the vocabulary as a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a final step, we use the json file to instantiate an object of the `Wav2Vec2CTCTokenizer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"data/vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create the feature extractor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create XLSR-Wav2Vec2 Feature Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech is a continuous signal and to be treated by computers, it first has to be discretized, which is usually called **sampling**. The sampling rate hereby plays an important role in that it defines how many data points of the speech signal are measured per second. Therefore, sampling with a higher sampling rate results in a better approximation of the *real* speech signal but also necessitates more values per second.\n",
    "\n",
    "A pretrained checkpoint expects its input data to have been sampled more or less from the same distribution as the data it was trained on. The same speech signals sampled at two different rates have a very different distribution, *e.g.*, doubling the sampling rate results in data points being twice as long. Thus, \n",
    "before fine-tuning a pretrained checkpoint of an ASR model, it is crucial to verify that the sampling rate of the data that was used to pretrain the model matches the sampling rate of the dataset used to fine-tune the model.\n",
    "\n",
    "XLSR-Wav2Vec2 was pretrained on the audio data of [Babel](https://huggingface.co/datasets/librispeech_asr), \n",
    "[Multilingual LibriSpeech (MLS)](https://ai.facebook.com/blog/a-new-open-data-set-for-multilingual-speech-research/), and [Common Voice](https://huggingface.co/datasets/common_voice). Most of those datasets were sampled at 16kHz, so that Common Voice, sampled at 48kHz, has to be downsampled to 16kHz for training. Therefore, we will have to downsample our fine-tuning data to 16kHz in the following.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A XLSR-Wav2Vec2 feature extractor object requires the following parameters to be instantiated:\n",
    "\n",
    "- `feature_size`: Speech models take a sequence of feature vectors as an input. While the length of this sequence obviously varies, the feature size should not. In the case of Wav2Vec2, the feature size is 1 because the model was trained on the raw speech signal ${}^2$.\n",
    "- `sampling_rate`: The sampling rate at which the model is trained on.\n",
    "- `padding_value`: For batched inference, shorter inputs need to be padded with a specific value\n",
    "- `do_normalize`: Whether the input should be *zero-mean-unit-variance* normalized or not. Usually, speech models perform better when normalizing the input\n",
    "- `return_attention_mask`: Whether the model should make use of an `attention_mask` for batched inference. In general, XLSR-Wav2Vec2 models should **always** make use of the `attention_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, XLSR-Wav2Vec2's feature extraction pipeline is thereby fully defined!\n",
    "\n",
    "To make the usage of XLSR-Wav2Vec2 as user-friendly as possible, the feature extractor and tokenizer are *wrapped* into a single `Wav2Vec2Processor` class so that one only needs a `model` and `processor` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one wants to re-use the just created processor and the fine-tuned model of this notebook, one can mount his/her google drive to the notebook and save all relevant files there. To do so, please uncomment the following lines. \n",
    "\n",
    "We will give the fine-tuned model the name `\"wav2vec2-large-xlsr-turkish-demo\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor.save_pretrained(\"/content/gdrive/MyDrive/wav2vec2-large-xlsr-turkish-demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can prepare the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "\n",
    "So far, we have not looked at the actual values of the speech signal but just kept the path to its file in the dataset. `XLSR-Wav2Vec2` expects the audio file in the format of a 1-dimensional array, so in the first step, let's load all audio files into the dataset object.\n",
    "\n",
    "Let's first check the serialization format of the downloaded audio files by looking at the first training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'data/downloads/extracted/65d4e7ba020098900e033dfe98c4ffcc447ffb05b35cc57059b5eda3c133023a/cv-corpus-6.1-2020-12-11/en/clips/common_voice_en_19018227.mp3',\n",
       " 'sentence': 'he promotes british industry and commerce both in the united states and at home '}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, the audio file is saved in the `.mp3` format. The `.mp3` format is usually not the easiest format to deal with. We found that the [`torchaudio`](https://pytorch.org/audio/stable/index.html) library works best for reading in `.mp3` data. \n",
    "\n",
    "An audio file usually stores both its values and the sampling rate with which the speech signal was digitalized. We want to store both in the dataset and write a `map(...)` function accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "def speech_file_to_array_fn(batch):\n",
    "    try:\n",
    "        speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "        batch[\"speech\"] = speech_array[0].numpy()\n",
    "        batch[\"sampling_rate\"] = sampling_rate\n",
    "    except:\n",
    "        batch[\"speech\"] = np.array([0])\n",
    "        batch[\"sampling_rate\"] = 0\n",
    "            \n",
    "    batch[\"target_text\"] = batch[\"sentence\"]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_voice_train = common_voice_train.map(speech_file_to_array_fn, remove_columns=common_voice_train.column_names)\n",
    "# common_voice_test = common_voice_test.map(speech_file_to_array_fn, remove_columns=common_voice_test.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loud Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import pyloudnorm as pyln\n",
    "\n",
    "def get_loudness_normalised(sa, sr):\n",
    "    # peak normalize audio to -1 dB\n",
    "    peak_normalized_audio = pyln.normalize.peak(sa, -1.0)\n",
    "\n",
    "    # measure the loudness first \n",
    "    meter = pyln.Meter(sr) # create BS.1770 meter\n",
    "    loudness = meter.integrated_loudness(sa)\n",
    "\n",
    "    # loudness normalize audio to -12 dB LUFS\n",
    "    loudness_normalized_audio = pyln.normalize.loudness(sa, loudness, -12.0)\n",
    "\n",
    "    return loudness_normalized_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_loud_norm_fn(batch):\n",
    "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "    \n",
    "    # DO loudness normalisation\n",
    "    sa = get_loudness_normalised(speech_array[0].numpy(), sampling_rate)\n",
    "    \n",
    "    batch[\"speech\"] = sa\n",
    "    batch[\"sampling_rate\"] = sampling_rate\n",
    "    batch[\"target_text\"] = batch[\"sentence\"]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['path', 'sentence'],\n",
       "    num_rows: 5637\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cpus = os.cpu_count(); n_cpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62bcdeda493d4ba9b341b65f49532faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "common_voice_train = common_voice_train.flatten_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583762556b304bd59fa20023e0a51869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5637.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "common_voice_train = common_voice_train.map(speech_file_to_array_fn, keep_in_memory=True, num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4917e2d0e9ed497b85e034fd718d9825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#1', max=64.0, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088a89f4dd1641cbb05ddc2aaa25841c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#0', max=64.0, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26315883dd1e41278ae685c3d22ce0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#4', max=63.0, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1eb84143b74449388c3fbad3486cb48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#3', max=63.0, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91f4e7f775745eebcea84e6050b2a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#7', max=63.0, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45cdbc8b6d6b48cc93e5e32c059d0217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#2', max=63.0, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488fa4d23a7f4c33b123e8f5850c5a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#5', max=63.0, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9271d894d6c842b89a5d068552bfe6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#6', max=63.0, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "common_voice_test = common_voice_test.map(speech_file_to_array_fn, num_proc=8, keep_in_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only normalise Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_voice_train = common_voice_train.map(speech_file_to_array_loud_norm_fn)\n",
    "# common_voice_test = common_voice_test.map(speech_file_to_array_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we've successfully read in all the audio files, but since we know that Common Voice is sampled at 48kHz, we need to resample the audio files to 16kHz. \n",
    "\n",
    "Let's make use of the [`librosa`](https://github.com/librosa/librosa) library to downsample the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def resample(batch):\n",
    "    batch[\"speech\"] = librosa.resample(np.asarray(batch[\"speech\"]), 48_000, 16_000)\n",
    "    batch[\"sampling_rate\"] = 16_000\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437bb8f974ac43b9a6702ef29cb9acf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5637.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c77b8c255b74e4a8332ee87680c37e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=506.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "common_voice_train = common_voice_train.map(resample)\n",
    "common_voice_test = common_voice_test.map(resample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seemed to have worked! Let's listen to a couple of audio files to better understand the dataset and verify that the audio was correctly loaded. \n",
    "\n",
    "**Note**: *You can click the following cell a couple of times to listen to different speech samples.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "rand_int = random.randint(0, len(common_voice_train)-1)\n",
    "\n",
    "ipd.Audio(data=np.asarray(common_voice_train[rand_int][\"speech\"]), autoplay=True, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be heard, that the speakers change along with their speaking rate, accent, and background environment, etc. Overall, the recordings sound acceptably clear though, which is to be expected from a crowd-sourced read speech corpus.\n",
    "\n",
    "Let's do a final check that the data is correctly prepared, by printing the shape of the speech input, its transcription, and the corresponding sampling rate.\n",
    "\n",
    "**Note**: *You can click the following cell a couple of times to verify multiple samples.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target text: that aroused suspicion the next day after lincoln was assassinated \n",
      "Input array shape: (112512,)\n",
      "Sampling rate: 16000\n"
     ]
    }
   ],
   "source": [
    "rand_int = random.randint(0, len(common_voice_train)-1)\n",
    "\n",
    "print(\"Target text:\", common_voice_train[rand_int][\"target_text\"])\n",
    "print(\"Input array shape:\", np.asarray(common_voice_train[rand_int][\"speech\"]).shape)\n",
    "print(\"Sampling rate:\", common_voice_train[rand_int][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Everything looks fine - the data is a 1-dimensional array, the sampling rate always corresponds to 16kHz, and the target text is normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can process the dataset to the format expected by the model for training. We will again make use of the `map(...)` function.\n",
    "\n",
    "First, we check that the data samples have the same sampling rate of 16kHz.\n",
    "Second, we extract the `input_values` from the loaded audio file. In our case, this includes only normalization, but for other speech models, this step could correspond to extracting, *e.g.* [Log-Mel features](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum). \n",
    "Third, we encode the transcriptions to label ids.\n",
    "\n",
    "**Note**: This mapping function is a good example of how the `Wav2Vec2Processor` class should be used. In \"normal\" context, calling `processor(...)` is redirected to `Wav2Vec2FeatureExtractor`'s call method. When wrapping the processor into the `as_target_processor` context, however, the same method is redirected to `Wav2Vec2CTCTokenizer`'s call method.\n",
    "For more information please check the [docs](https://huggingface.co/transformers/master/model_doc/wav2vec2.html#transformers.Wav2Vec2Processor.__call__)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # check that all files have the correct sampling rate\n",
    "    assert (\n",
    "        len(set(batch[\"sampling_rate\"])) == 1\n",
    "    ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n",
    "\n",
    "    batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]).input_values\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "'i' format requires -2147483648 <= number <= 2147483647",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-04d04d6d5766>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcommon_voice_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommon_voice_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcommon_voice_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_proc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcommon_voice_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommon_voice_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcommon_voice_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_proc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Spawning {} processes\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_proc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwds_per_shard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                 \u001b[0mtransformed_shards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1486\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Concatenating {} shards from multiprocessing\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_proc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_shards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Spawning {} processes\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_proc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwds_per_shard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                 \u001b[0mtransformed_shards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1486\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Concatenating {} shards from multiprocessing\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_proc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_shards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36m_handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    429\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m                         \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m                         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/multiprocess/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/multiprocess/connection.py\u001b[0m in \u001b[0;36m_send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;31m# For wire compatibility with 3.2 and lower\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m16384\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;31m# The payload is large so Nagle's algorithm won't be triggered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: 'i' format requires -2147483648 <= number <= 2147483647"
     ]
    }
   ],
   "source": [
    "common_voice_train = common_voice_train.map(prepare_dataset, remove_columns=common_voice_train.column_names, batch_size=8, num_proc=4, batched=True)\n",
    "common_voice_test = common_voice_test.map(prepare_dataset, remove_columns=common_voice_test.column_names, batch_size=8, num_proc=4, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train.save_to_disk('data/common_voice_train_ga-en_5637')\n",
    "common_voice_test.save_to_disk('data/common_voice_test_ga-en_5637')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The data is processed so that we are ready to start setting up the training pipeline. We will make use of 🤗's [Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer) for which we essentially need to do the following:\n",
    "\n",
    "- Define a data collator. In contrast to most NLP models, XLSR-Wav2Vec2 has a much larger input length than output length. *E.g.*, a sample of input length 50000 has an output length of no more than 100. Given the large input sizes, it is much more efficient to pad the training batches dynamically meaning that all training samples should only be padded to the longest sample in their batch and not the overall longest sample. Therefore, fine-tuning XLSR-Wav2Vec2 requires a special padding data collator, which we will define below\n",
    "\n",
    "- Evaluation metric. During training, the model should be evaluated on the word error rate. We should define a `compute_metrics` function accordingly\n",
    "\n",
    "- Load a pretrained checkpoint. We need to load a pretrained checkpoint and configure it correctly for training.\n",
    "\n",
    "- Define the training configuration.\n",
    "\n",
    "After having fine-tuned the model, we will correctly evaluate it on the test data and verify that it has indeed learned to correctly transcribe speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up Trainer\n",
    "\n",
    "Let's start by defining the data collator. The code for the data collator was copied from [this example](https://github.com/huggingface/transformers/blob/9a06b6b11bdfc42eea08fa91d0c737d1863c99e3/examples/research_projects/wav2vec2/run_asr.py#L81).\n",
    "\n",
    "Without going into too many details, in contrast to the common data collators, this data collator treats the `input_values` and `labels` differently and thus applies to separate padding functions on them (again making use of XLSR-Wav2Vec2's context manager). This is necessary because in speech input and output are of different modalities meaning that they should not be treated by the same padding function.\n",
    "Analogous to the common data collators, the padding tokens in the labels with `-100` so that those tokens are **not** taken into account when computing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True,\n",
    "                                          pad_to_multiple_of=8, pad_to_multiple_of_labels=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the evaluation metric is defined. As mentioned earlier, the \n",
    "predominant metric in ASR is the word error rate (WER), hence we will use it in this notebook as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will return a sequence of logit vectors:\n",
    "$\\mathbf{y}_1, \\ldots, \\mathbf{y}_m$ with $\\mathbf{y}_1 = f_{\\theta}(x_1, \\ldots, x_n)[0]$ and $n >> m$.\n",
    "\n",
    "A logit vector $\\mathbf{y}_1$ contains the log-odds for each word in the vocabulary we defined earlier, thus $\\text{len}(\\mathbf{y}_i) =$ `config.vocab_size`. We are interested in the most likely prediction of the model and thus take the `argmax(...)` of the logits. Also, we transform the encoded labels back to the original string by replacing `-100` with the `pad_token_id` and decoding the ids while making sure that consecutive tokens are **not** grouped to the same token in CTC style ${}^1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the pretrained `XLSR-Wav2Vec2` checkpoint. The tokenizer's `pad_token_id` must be to define the model's `pad_token_id` or in the case of `Wav2Vec2ForCTC` also CTC's *blank token* ${}^2$. To save GPU memory, we enable PyTorch's [gradient checkpointing](https://pytorch.org/docs/stable/checkpoint.html) and also set the loss reduction to \"*mean*\".\n",
    "\n",
    "Because the dataset is quite small (~6h of training data) and because Common Voice is quite noisy, fine-tuning Facebook's [wav2vec2-large-xlsr-53 checkpoint](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) seems to require some hyper-parameter tuning. Therefore, I had to play around a bit with different values for dropout, [SpecAugment](https://arxiv.org/abs/1904.08779)'s masking dropout rate, layer dropout, and the learning rate until training seemed to be stable enough. \n",
    "\n",
    "**Note**: When using this notebook to train XLSR-Wav2Vec2 on another language of Common Voice those hyper-parameter settings might not work very well. Feel free to adapt those depending on your use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-xlsr-53\", \n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.1,\n",
    "    gradient_checkpointing=True, \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first component of XLSR-Wav2Vec2 consists of a stack of CNN layers that are used to extract acoustically meaningful - but contextually independent - features from the raw speech signal. This part of the model has already been sufficiently trained during pretraining and as stated in the [paper](https://arxiv.org/pdf/2006.13979.pdf) does not need to be fine-tuned anymore. \n",
    "Thus, we can set the `requires_grad` to `False` for all parameters of the *feature extraction* part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a final step, we define all parameters related to training. \n",
    "To give more explanation on some of the parameters:\n",
    "- `group_by_length` makes training more efficient by grouping training samples of similar input length into one batch. This can significantly speed up training time by heavily reducing the overall number of useless padding tokens that are passed through the model\n",
    "- `learning_rate` and `weight_decay` were heuristically tuned until fine-tuning has become stable. Note that those parameters strongly depend on the Common Voice dataset and might be suboptimal for other speech datasets.\n",
    "\n",
    "For more explanations on other parameters, one can take a look at the [docs](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer#trainingarguments).\n",
    "\n",
    "**Note**: If one wants to save the trained models in his/her google drive the commented-out `output_dir` can be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"data\",\n",
    "  # output_dir=\"./wav2vec2-large-xlsr-turkish-demo\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=32,\n",
    "  per_device_eval_batch_size=64,\n",
    "  gradient_accumulation_steps=1,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=50,\n",
    "  fp16=True,\n",
    "  save_steps=25,\n",
    "  eval_steps=25,\n",
    "  logging_steps=5,\n",
    "  learning_rate=3e-4,\n",
    "  warmup_steps=200,\n",
    "  save_total_limit=1,\n",
    "    \n",
    "  # WANDB LOGGING: \n",
    "  report_to = 'wandb',  # enable logging to W&B\n",
    "  run_name = 'ie-base-50e-ovh-4-4-upgrade',   # Name your run, optional\n",
    "  load_best_model_at_end = True,  # This will ensure your best model will be uploaded to W&B\n",
    "  metric_for_best_model='wer',    # Load best model based on \"wer\", not eval loss\n",
    "  greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all instances can be passed to Trainer and we are ready to start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=common_voice_train,\n",
    "    eval_dataset=common_voice_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "${}^1$ To allow models to become independent of the speaker rate, in CTC, consecutive tokens that are identical are simply grouped as a single token. However, the encoded labels should not be grouped when decoding since they don't correspond to the predicted tokens of the model, which is why the `group_tokens=False` parameter has to be passed. If we wouldn't pass this parameter a word like `\"hello\"` would incorrectly be encoded, and decoded as `\"helo\"`.\n",
    "\n",
    "${}^2$ The blank token allows the model to predict a word, such as `\"hello\"` by forcing it to insert the blank token between the two l's. A CTC-conform prediction of `\"hello\"` of our model would be `[PAD] [PAD] \"h\" \"e\" \"e\" \"l\" \"l\" [PAD] \"l\" \"o\" \"o\" [PAD]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training will take between 180 and 240 minutes depending on the GPU allocated to this notebook. While the trained model yields somewhat satisfying results on *Common Voice*'s test data of Turkish, it is by no means an optimally fine-tuned model. The purpose of this notebook is to demonstrate how XLSR-Wav2Vec2's [checkpoint](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) can be fine-tuned on a low-resource ASR dataset.\n",
    "\n",
    "In case you want to use this google colab to fine-tune your model, you should make sure that your training doesn't stop due to inactivity. A simple hack to prevent this is to paste the following code into the console of this tab (*right mouse click -> inspect -> Console tab and insert code*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```javascript\n",
    "function ConnectButton(){\n",
    "    console.log(\"Connect pushed\"); \n",
    "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
    "}\n",
    "setInterval(ConnectButton,60000);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                                        Size  Used Avail Use% Mounted on\n",
      "overlay                                           388G  126G  263G  33% /\n",
      "tmpfs                                              64M     0   64M   0% /dev\n",
      "tmpfs                                              87G     0   87G   0% /sys/fs/cgroup\n",
      "/dev/sda1                                         388G  126G  263G  33% /home\n",
      "10.98.115.129,10.97.15.129,10.97.7.129:/ovh/data  8.6T  274G  8.3T   4% /workspace/data\n",
      "tmpfs                                              87G   56K   87G   1% /dev/shm\n",
      "tmpfs                                              87G   12K   87G   1% /proc/driver/nvidia\n",
      "udev                                               87G     0   87G   0% /dev/nvidia1\n",
      "tmpfs                                              87G     0   87G   0% /proc/acpi\n",
      "tmpfs                                              87G     0   87G   0% /proc/scsi\n",
      "tmpfs                                              87G     0   87G   0% /sys/firmware\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">ie-base-50e-ovh-4-4-upgrade</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/wandb/xlsr-irish\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/wandb/xlsr-irish/runs/17jq6qjl\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish/runs/17jq6qjl</a><br/>\n",
       "                Run data is saved locally in <code>/workspace/wandb/run-20210322_213555-17jq6qjl</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1650' max='1650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1650/1650 2:25:59, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>16.910900</td>\n",
       "      <td>19.851835</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.199500</td>\n",
       "      <td>10.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>11.013100</td>\n",
       "      <td>9.274652</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>49.676000</td>\n",
       "      <td>10.186000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>3.909600</td>\n",
       "      <td>3.731420</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>49.708100</td>\n",
       "      <td>10.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.116000</td>\n",
       "      <td>3.093797</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>49.602700</td>\n",
       "      <td>10.201000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>3.027200</td>\n",
       "      <td>3.029405</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.286700</td>\n",
       "      <td>10.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.985200</td>\n",
       "      <td>2.969006</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.159000</td>\n",
       "      <td>10.088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>2.995400</td>\n",
       "      <td>2.992875</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>49.755800</td>\n",
       "      <td>10.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.964600</td>\n",
       "      <td>2.920878</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>51.081200</td>\n",
       "      <td>9.906000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>2.918800</td>\n",
       "      <td>2.924300</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>49.780900</td>\n",
       "      <td>10.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.922700</td>\n",
       "      <td>2.950752</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.190600</td>\n",
       "      <td>10.082000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.897700</td>\n",
       "      <td>2.892996</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>49.875900</td>\n",
       "      <td>10.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.977900</td>\n",
       "      <td>2.902837</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.682500</td>\n",
       "      <td>9.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>2.888700</td>\n",
       "      <td>2.866564</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>51.886900</td>\n",
       "      <td>9.752000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.746400</td>\n",
       "      <td>2.635600</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>49.897400</td>\n",
       "      <td>10.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>2.425900</td>\n",
       "      <td>2.277162</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>52.704200</td>\n",
       "      <td>9.601000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.892100</td>\n",
       "      <td>1.785602</td>\n",
       "      <td>0.984786</td>\n",
       "      <td>50.220700</td>\n",
       "      <td>10.076000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.633600</td>\n",
       "      <td>1.382392</td>\n",
       "      <td>0.955822</td>\n",
       "      <td>50.287000</td>\n",
       "      <td>10.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.298900</td>\n",
       "      <td>1.184137</td>\n",
       "      <td>0.829432</td>\n",
       "      <td>54.454600</td>\n",
       "      <td>9.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>1.131942</td>\n",
       "      <td>0.787303</td>\n",
       "      <td>55.295700</td>\n",
       "      <td>9.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.866400</td>\n",
       "      <td>1.047049</td>\n",
       "      <td>0.768578</td>\n",
       "      <td>54.032200</td>\n",
       "      <td>9.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.697700</td>\n",
       "      <td>1.055920</td>\n",
       "      <td>0.743417</td>\n",
       "      <td>55.441700</td>\n",
       "      <td>9.127000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.716200</td>\n",
       "      <td>1.031850</td>\n",
       "      <td>0.735810</td>\n",
       "      <td>50.515700</td>\n",
       "      <td>10.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.461400</td>\n",
       "      <td>1.014104</td>\n",
       "      <td>0.690755</td>\n",
       "      <td>50.152200</td>\n",
       "      <td>10.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.499600</td>\n",
       "      <td>1.025969</td>\n",
       "      <td>0.681978</td>\n",
       "      <td>51.743700</td>\n",
       "      <td>9.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.379600</td>\n",
       "      <td>1.054192</td>\n",
       "      <td>0.708602</td>\n",
       "      <td>50.723500</td>\n",
       "      <td>9.976000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.308800</td>\n",
       "      <td>1.057128</td>\n",
       "      <td>0.686366</td>\n",
       "      <td>50.325800</td>\n",
       "      <td>10.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.274300</td>\n",
       "      <td>1.151791</td>\n",
       "      <td>0.687829</td>\n",
       "      <td>50.590800</td>\n",
       "      <td>10.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.194600</td>\n",
       "      <td>1.143884</td>\n",
       "      <td>0.674078</td>\n",
       "      <td>50.980900</td>\n",
       "      <td>9.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.262700</td>\n",
       "      <td>1.088287</td>\n",
       "      <td>0.640140</td>\n",
       "      <td>50.745000</td>\n",
       "      <td>9.971000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.165900</td>\n",
       "      <td>1.117724</td>\n",
       "      <td>0.638678</td>\n",
       "      <td>51.238600</td>\n",
       "      <td>9.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.150400</td>\n",
       "      <td>1.058805</td>\n",
       "      <td>0.629901</td>\n",
       "      <td>51.105900</td>\n",
       "      <td>9.901000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.154500</td>\n",
       "      <td>1.117424</td>\n",
       "      <td>0.638385</td>\n",
       "      <td>50.740500</td>\n",
       "      <td>9.972000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.142600</td>\n",
       "      <td>1.138583</td>\n",
       "      <td>0.643651</td>\n",
       "      <td>50.760400</td>\n",
       "      <td>9.968000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.157100</td>\n",
       "      <td>1.100966</td>\n",
       "      <td>0.630193</td>\n",
       "      <td>51.914500</td>\n",
       "      <td>9.747000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.155900</td>\n",
       "      <td>1.154825</td>\n",
       "      <td>0.620831</td>\n",
       "      <td>51.154500</td>\n",
       "      <td>9.892000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.100600</td>\n",
       "      <td>1.208675</td>\n",
       "      <td>0.631363</td>\n",
       "      <td>51.126300</td>\n",
       "      <td>9.897000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>1.168609</td>\n",
       "      <td>0.632241</td>\n",
       "      <td>53.086600</td>\n",
       "      <td>9.532000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.124300</td>\n",
       "      <td>1.202987</td>\n",
       "      <td>0.626097</td>\n",
       "      <td>51.424600</td>\n",
       "      <td>9.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.145700</td>\n",
       "      <td>1.183988</td>\n",
       "      <td>0.615272</td>\n",
       "      <td>51.485800</td>\n",
       "      <td>9.828000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>1.211649</td>\n",
       "      <td>0.617905</td>\n",
       "      <td>53.008000</td>\n",
       "      <td>9.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.111600</td>\n",
       "      <td>1.232898</td>\n",
       "      <td>0.617613</td>\n",
       "      <td>51.768600</td>\n",
       "      <td>9.774000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.117500</td>\n",
       "      <td>1.198322</td>\n",
       "      <td>0.603862</td>\n",
       "      <td>51.660600</td>\n",
       "      <td>9.795000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.119500</td>\n",
       "      <td>1.210388</td>\n",
       "      <td>0.604740</td>\n",
       "      <td>51.189000</td>\n",
       "      <td>9.885000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>1.244329</td>\n",
       "      <td>0.610591</td>\n",
       "      <td>51.254300</td>\n",
       "      <td>9.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.153600</td>\n",
       "      <td>1.201496</td>\n",
       "      <td>0.603862</td>\n",
       "      <td>51.552000</td>\n",
       "      <td>9.815000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.149600</td>\n",
       "      <td>1.330979</td>\n",
       "      <td>0.614102</td>\n",
       "      <td>51.871300</td>\n",
       "      <td>9.755000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>0.082100</td>\n",
       "      <td>1.274745</td>\n",
       "      <td>0.608543</td>\n",
       "      <td>51.746700</td>\n",
       "      <td>9.778000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.091700</td>\n",
       "      <td>1.293233</td>\n",
       "      <td>0.620831</td>\n",
       "      <td>52.096700</td>\n",
       "      <td>9.713000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>0.079200</td>\n",
       "      <td>1.311805</td>\n",
       "      <td>0.614980</td>\n",
       "      <td>51.488800</td>\n",
       "      <td>9.827000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.085500</td>\n",
       "      <td>1.262385</td>\n",
       "      <td>0.598303</td>\n",
       "      <td>52.241100</td>\n",
       "      <td>9.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.112300</td>\n",
       "      <td>1.259497</td>\n",
       "      <td>0.601814</td>\n",
       "      <td>51.024400</td>\n",
       "      <td>9.917000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>1.262543</td>\n",
       "      <td>0.596255</td>\n",
       "      <td>53.089400</td>\n",
       "      <td>9.531000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>0.167000</td>\n",
       "      <td>1.270383</td>\n",
       "      <td>0.596255</td>\n",
       "      <td>53.679500</td>\n",
       "      <td>9.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>1.286125</td>\n",
       "      <td>0.599473</td>\n",
       "      <td>53.632200</td>\n",
       "      <td>9.435000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>1.300773</td>\n",
       "      <td>0.599766</td>\n",
       "      <td>52.186700</td>\n",
       "      <td>9.696000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>1.314763</td>\n",
       "      <td>0.602692</td>\n",
       "      <td>51.011000</td>\n",
       "      <td>9.919000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>1.315718</td>\n",
       "      <td>0.590696</td>\n",
       "      <td>52.855800</td>\n",
       "      <td>9.573000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>1.312548</td>\n",
       "      <td>0.602984</td>\n",
       "      <td>52.109800</td>\n",
       "      <td>9.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>1.315958</td>\n",
       "      <td>0.600059</td>\n",
       "      <td>51.060300</td>\n",
       "      <td>9.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.059700</td>\n",
       "      <td>1.295234</td>\n",
       "      <td>0.596840</td>\n",
       "      <td>51.144200</td>\n",
       "      <td>9.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>1.290017</td>\n",
       "      <td>0.590111</td>\n",
       "      <td>51.837800</td>\n",
       "      <td>9.761000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>1.300187</td>\n",
       "      <td>0.591281</td>\n",
       "      <td>52.421800</td>\n",
       "      <td>9.652000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>1.299748</td>\n",
       "      <td>0.592452</td>\n",
       "      <td>52.198700</td>\n",
       "      <td>9.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>1.301730</td>\n",
       "      <td>0.591867</td>\n",
       "      <td>51.750500</td>\n",
       "      <td>9.778000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>0.086500</td>\n",
       "      <td>1.294062</td>\n",
       "      <td>0.592744</td>\n",
       "      <td>52.159500</td>\n",
       "      <td>9.701000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>1.296774</td>\n",
       "      <td>0.593915</td>\n",
       "      <td>52.041200</td>\n",
       "      <td>9.723000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1650, training_loss=1.3173199103456554, metrics={'train_runtime': 8759.7664, 'train_samples_per_second': 0.188, 'total_flos': 6.579265506797722e+18, 'epoch': 50.0, 'init_mem_cpu_alloc_delta': 58580, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 18306, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 0, 'train_mem_gpu_alloc_delta': 5058206720, 'train_mem_cpu_peaked_delta': 0, 'train_mem_gpu_peaked_delta': 0})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38215<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 1203.68MB of 1203.68MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/workspace/wandb/run-20210322_213555-17jq6qjl/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/workspace/wandb/run-20210322_213555-17jq6qjl/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.0835</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/epoch</td><td>50.0</td></tr><tr><td>_runtime</td><td>8760</td></tr><tr><td>_timestamp</td><td>1616457715</td></tr><tr><td>_step</td><td>1650</td></tr><tr><td>eval/loss</td><td>1.29677</td></tr><tr><td>eval/wer</td><td>0.59391</td></tr><tr><td>eval/runtime</td><td>52.0412</td></tr><tr><td>eval/samples_per_second</td><td>9.723</td></tr><tr><td>train/train_runtime</td><td>8759.7664</td></tr><tr><td>train/train_samples_per_second</td><td>0.188</td></tr><tr><td>train/total_flos</td><td>6.579265506797722e+18</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▂▃▅▆▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>█▄▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/wer</td><td>██████████▇▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▂▁▁▂▁▁▁▂▁▂▂██▂▄▂▂▂▃▂▄▃▅▃▄▃▃▄▄▃▃▅▆▃▅▃▄▄▄▄</td></tr><tr><td>eval/samples_per_second</td><td>▇██▇███▇█▇▇▁▁▇▅▇▇▇▆▇▅▆▄▆▅▅▆▅▅▆▆▄▃▆▄▆▅▄▅▅</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">ie-base-50e-ovh-4-4-upgrade</strong>: <a href=\"https://wandb.ai/wandb/xlsr-irish/runs/17jq6qjl\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish/runs/17jq6qjl</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                                        Size  Used Avail Use% Mounted on\n",
      "overlay                                           388G  147G  241G  38% /\n",
      "tmpfs                                              64M     0   64M   0% /dev\n",
      "tmpfs                                              87G     0   87G   0% /sys/fs/cgroup\n",
      "/dev/sda1                                         388G  147G  241G  38% /home\n",
      "10.98.115.129,10.97.15.129,10.97.7.129:/ovh/data  8.6T  363G  8.2T   5% /workspace/data\n",
      "tmpfs                                              87G  128K   87G   1% /dev/shm\n",
      "tmpfs                                              87G   12K   87G   1% /proc/driver/nvidia\n",
      "udev                                               87G     0   87G   0% /dev/nvidia1\n",
      "tmpfs                                              87G     0   87G   0% /proc/acpi\n",
      "tmpfs                                              87G     0   87G   0% /proc/scsi\n",
      "tmpfs                                              87G     0   87G   0% /sys/firmware\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loss goes down and we can see that the WER on the test set also improves nicely. Because this notebook is just for demonstration purposes, we can stop here.\n",
    "\n",
    "The resulting model of this notebook has been saved to [`patrickvonplaten/wav2vec2-large-xlsr-turkish-demo`](https://huggingface.co/patrickvonplaten/wav2vec2-large-xlsr-turkish-demo)\n",
    "\n",
    "As a final check, let's load the model and verify that it indeed has learned to transcribe Turkish speech.\n",
    "\n",
    "Let's first load the pretrained checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-xlsr-53\", \n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.1,\n",
    "    gradient_checkpointing=True, \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=\"data\",\n",
    "  # output_dir=\"./wav2vec2-large-xlsr-turkish-demo\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=32,\n",
    "  per_device_eval_batch_size=64,\n",
    "  gradient_accumulation_steps=1,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=50,\n",
    "  fp16=True,\n",
    "  save_steps=25,\n",
    "  eval_steps=25,\n",
    "  logging_steps=5,\n",
    "  learning_rate=1e-4,\n",
    "  warmup_steps=100,\n",
    "  save_total_limit=1,\n",
    "    \n",
    "  # WANDB LOGGING: \n",
    "  report_to = 'wandb',  # enable logging to W&B\n",
    "  run_name = 'ie1e-4-100w-50e-ovh-4-4-upgrade',   # Name your run, optional\n",
    "  load_best_model_at_end = True,  # This will ensure your best model will be uploaded to W&B\n",
    "  metric_for_best_model='wer',    # Load best model based on \"wer\", not eval loss\n",
    "  greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=common_voice_train,\n",
    "    eval_dataset=common_voice_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">ie1e-4-100w-50e-ovh-4-4-upgrade</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/wandb/xlsr-irish\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/wandb/xlsr-irish/runs/35m5a15b\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish/runs/35m5a15b</a><br/>\n",
       "                Run data is saved locally in <code>/workspace/wandb/run-20210323_000310-35m5a15b</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1650' max='1650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1650/1650 2:28:41, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>16.784000</td>\n",
       "      <td>19.620174</td>\n",
       "      <td>0.998245</td>\n",
       "      <td>51.626700</td>\n",
       "      <td>9.801000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>15.071400</td>\n",
       "      <td>18.899715</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>49.470100</td>\n",
       "      <td>10.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>8.792200</td>\n",
       "      <td>7.581682</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.218100</td>\n",
       "      <td>10.076000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.581600</td>\n",
       "      <td>3.452944</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.533300</td>\n",
       "      <td>10.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>3.100200</td>\n",
       "      <td>3.082635</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.991300</td>\n",
       "      <td>9.923000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.014100</td>\n",
       "      <td>3.002206</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>52.654900</td>\n",
       "      <td>9.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>2.968600</td>\n",
       "      <td>2.963183</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>49.323200</td>\n",
       "      <td>10.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.995100</td>\n",
       "      <td>2.944443</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>49.598100</td>\n",
       "      <td>10.202000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>2.930400</td>\n",
       "      <td>2.934498</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>49.725900</td>\n",
       "      <td>10.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.937100</td>\n",
       "      <td>2.929278</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.130900</td>\n",
       "      <td>10.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.916400</td>\n",
       "      <td>2.931020</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.412100</td>\n",
       "      <td>10.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.927800</td>\n",
       "      <td>2.914973</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.497800</td>\n",
       "      <td>10.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>2.913700</td>\n",
       "      <td>2.930453</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>52.416500</td>\n",
       "      <td>9.653000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.911500</td>\n",
       "      <td>2.903660</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.520100</td>\n",
       "      <td>10.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>2.909300</td>\n",
       "      <td>2.901131</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>51.772500</td>\n",
       "      <td>9.774000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.869100</td>\n",
       "      <td>2.880433</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>49.606200</td>\n",
       "      <td>10.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>2.869800</td>\n",
       "      <td>2.859838</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>49.779500</td>\n",
       "      <td>10.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.834800</td>\n",
       "      <td>2.813498</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>51.145000</td>\n",
       "      <td>9.893000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>2.730000</td>\n",
       "      <td>2.624994</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>49.714200</td>\n",
       "      <td>10.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.519400</td>\n",
       "      <td>2.364450</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>49.702200</td>\n",
       "      <td>10.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>2.170600</td>\n",
       "      <td>2.028677</td>\n",
       "      <td>0.987712</td>\n",
       "      <td>49.772800</td>\n",
       "      <td>10.166000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.944500</td>\n",
       "      <td>1.758777</td>\n",
       "      <td>1.013166</td>\n",
       "      <td>49.829900</td>\n",
       "      <td>10.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.542758</td>\n",
       "      <td>0.963429</td>\n",
       "      <td>49.900000</td>\n",
       "      <td>10.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.473300</td>\n",
       "      <td>1.406157</td>\n",
       "      <td>0.932709</td>\n",
       "      <td>49.908200</td>\n",
       "      <td>10.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>1.285400</td>\n",
       "      <td>1.300028</td>\n",
       "      <td>0.882680</td>\n",
       "      <td>49.840800</td>\n",
       "      <td>10.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.116600</td>\n",
       "      <td>1.243172</td>\n",
       "      <td>0.864833</td>\n",
       "      <td>49.828900</td>\n",
       "      <td>10.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>1.041200</td>\n",
       "      <td>1.191273</td>\n",
       "      <td>0.843768</td>\n",
       "      <td>49.978400</td>\n",
       "      <td>10.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.897800</td>\n",
       "      <td>1.168799</td>\n",
       "      <td>0.807197</td>\n",
       "      <td>49.913700</td>\n",
       "      <td>10.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.982100</td>\n",
       "      <td>1.122921</td>\n",
       "      <td>0.785840</td>\n",
       "      <td>50.242500</td>\n",
       "      <td>10.071000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.763900</td>\n",
       "      <td>1.096007</td>\n",
       "      <td>0.789936</td>\n",
       "      <td>50.221500</td>\n",
       "      <td>10.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.750400</td>\n",
       "      <td>1.074960</td>\n",
       "      <td>0.763312</td>\n",
       "      <td>50.546400</td>\n",
       "      <td>10.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.704700</td>\n",
       "      <td>1.050267</td>\n",
       "      <td>0.741954</td>\n",
       "      <td>50.159000</td>\n",
       "      <td>10.088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.696600</td>\n",
       "      <td>1.066875</td>\n",
       "      <td>0.746050</td>\n",
       "      <td>50.127200</td>\n",
       "      <td>10.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.674800</td>\n",
       "      <td>1.045970</td>\n",
       "      <td>0.737273</td>\n",
       "      <td>50.309900</td>\n",
       "      <td>10.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.659300</td>\n",
       "      <td>1.075299</td>\n",
       "      <td>0.759801</td>\n",
       "      <td>50.076900</td>\n",
       "      <td>10.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.634800</td>\n",
       "      <td>1.042072</td>\n",
       "      <td>0.729666</td>\n",
       "      <td>50.361100</td>\n",
       "      <td>10.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.634900</td>\n",
       "      <td>1.023295</td>\n",
       "      <td>0.713283</td>\n",
       "      <td>50.203200</td>\n",
       "      <td>10.079000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>1.055638</td>\n",
       "      <td>0.722060</td>\n",
       "      <td>50.321700</td>\n",
       "      <td>10.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.548500</td>\n",
       "      <td>1.015181</td>\n",
       "      <td>0.710064</td>\n",
       "      <td>52.417200</td>\n",
       "      <td>9.653000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>1.018952</td>\n",
       "      <td>0.706554</td>\n",
       "      <td>50.215200</td>\n",
       "      <td>10.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.562200</td>\n",
       "      <td>1.036254</td>\n",
       "      <td>0.702750</td>\n",
       "      <td>50.309600</td>\n",
       "      <td>10.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.569600</td>\n",
       "      <td>1.022944</td>\n",
       "      <td>0.687829</td>\n",
       "      <td>52.623400</td>\n",
       "      <td>9.615000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.558700</td>\n",
       "      <td>1.018984</td>\n",
       "      <td>0.684026</td>\n",
       "      <td>50.330100</td>\n",
       "      <td>10.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.643200</td>\n",
       "      <td>1.007001</td>\n",
       "      <td>0.693681</td>\n",
       "      <td>50.413700</td>\n",
       "      <td>10.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.619000</td>\n",
       "      <td>1.037667</td>\n",
       "      <td>0.695436</td>\n",
       "      <td>50.574100</td>\n",
       "      <td>10.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.603900</td>\n",
       "      <td>1.074043</td>\n",
       "      <td>0.702750</td>\n",
       "      <td>52.563000</td>\n",
       "      <td>9.627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>1.023878</td>\n",
       "      <td>0.693973</td>\n",
       "      <td>52.577500</td>\n",
       "      <td>9.624000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.512200</td>\n",
       "      <td>1.031776</td>\n",
       "      <td>0.691925</td>\n",
       "      <td>52.467500</td>\n",
       "      <td>9.644000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>0.392300</td>\n",
       "      <td>1.019144</td>\n",
       "      <td>0.685781</td>\n",
       "      <td>50.337300</td>\n",
       "      <td>10.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.455200</td>\n",
       "      <td>1.020124</td>\n",
       "      <td>0.678760</td>\n",
       "      <td>50.369500</td>\n",
       "      <td>10.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.442900</td>\n",
       "      <td>1.023439</td>\n",
       "      <td>0.681685</td>\n",
       "      <td>51.129100</td>\n",
       "      <td>9.897000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.426900</td>\n",
       "      <td>1.019412</td>\n",
       "      <td>0.678174</td>\n",
       "      <td>50.380800</td>\n",
       "      <td>10.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>0.586800</td>\n",
       "      <td>1.051660</td>\n",
       "      <td>0.676419</td>\n",
       "      <td>50.409900</td>\n",
       "      <td>10.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.427100</td>\n",
       "      <td>1.052357</td>\n",
       "      <td>0.685489</td>\n",
       "      <td>50.418300</td>\n",
       "      <td>10.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>0.452600</td>\n",
       "      <td>1.050025</td>\n",
       "      <td>0.679345</td>\n",
       "      <td>50.677600</td>\n",
       "      <td>9.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.320200</td>\n",
       "      <td>1.048839</td>\n",
       "      <td>0.672030</td>\n",
       "      <td>50.399300</td>\n",
       "      <td>10.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.486300</td>\n",
       "      <td>1.036102</td>\n",
       "      <td>0.670568</td>\n",
       "      <td>52.678300</td>\n",
       "      <td>9.605000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.293700</td>\n",
       "      <td>1.050599</td>\n",
       "      <td>0.673201</td>\n",
       "      <td>52.650700</td>\n",
       "      <td>9.611000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>0.324900</td>\n",
       "      <td>1.043226</td>\n",
       "      <td>0.666179</td>\n",
       "      <td>52.896100</td>\n",
       "      <td>9.566000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.376400</td>\n",
       "      <td>1.048260</td>\n",
       "      <td>0.673786</td>\n",
       "      <td>50.514200</td>\n",
       "      <td>10.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>0.319300</td>\n",
       "      <td>1.042746</td>\n",
       "      <td>0.668520</td>\n",
       "      <td>52.584300</td>\n",
       "      <td>9.623000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.259700</td>\n",
       "      <td>1.047659</td>\n",
       "      <td>0.670860</td>\n",
       "      <td>50.808000</td>\n",
       "      <td>9.959000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>0.363600</td>\n",
       "      <td>1.039763</td>\n",
       "      <td>0.665301</td>\n",
       "      <td>52.764700</td>\n",
       "      <td>9.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.228400</td>\n",
       "      <td>1.045631</td>\n",
       "      <td>0.666179</td>\n",
       "      <td>50.747100</td>\n",
       "      <td>9.971000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>0.376400</td>\n",
       "      <td>1.048252</td>\n",
       "      <td>0.663839</td>\n",
       "      <td>51.526400</td>\n",
       "      <td>9.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>1.050394</td>\n",
       "      <td>0.666179</td>\n",
       "      <td>50.422700</td>\n",
       "      <td>10.035000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 38250<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 1203.68MB of 1203.68MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/workspace/wandb/run-20210323_000310-35m5a15b/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/workspace/wandb/run-20210323_000310-35m5a15b/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.3555</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/epoch</td><td>50.0</td></tr><tr><td>_runtime</td><td>8923</td></tr><tr><td>_timestamp</td><td>1616466713</td></tr><tr><td>_step</td><td>1650</td></tr><tr><td>eval/loss</td><td>1.05039</td></tr><tr><td>eval/wer</td><td>0.66618</td></tr><tr><td>eval/runtime</td><td>50.4227</td></tr><tr><td>eval/samples_per_second</td><td>10.035</td></tr><tr><td>train/train_runtime</td><td>8922.7199</td></tr><tr><td>train/train_samples_per_second</td><td>0.185</td></tr><tr><td>train/total_flos</td><td>6.579265506797722e+18</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>██▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▂▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>██▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/wer</td><td>████████████▇█▆▅▅▃▃▃▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▆▁▃█▁▂▃▃▃▂▂▂▂▂▂▂▂▃▃▃▃▃▃▇▃▇▃▇▇▃▅▃▃▃██▇▄▄▃</td></tr><tr><td>eval/samples_per_second</td><td>▃█▆▁█▇▆▆▆▇▇▇▇▇▇▇▇▆▅▆▆▆▆▂▆▁▆▂▂▆▄▆▆▆▁▁▂▅▅▆</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">ie1e-4-100w-50e-ovh-4-4-upgrade</strong>: <a href=\"https://wandb.ai/wandb/xlsr-irish/runs/35m5a15b\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish/runs/35m5a15b</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-xlsr-53\", \n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.1,\n",
    "    gradient_checkpointing=True, \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=\"data\",\n",
    "  # output_dir=\"./wav2vec2-large-xlsr-turkish-demo\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=48,\n",
    "  per_device_eval_batch_size=64,\n",
    "  gradient_accumulation_steps=1,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=50,\n",
    "  fp16=True,\n",
    "  save_steps=25,\n",
    "  eval_steps=25,\n",
    "  logging_steps=5,\n",
    "  learning_rate=1e-4,\n",
    "  warmup_steps=400,\n",
    "  save_total_limit=1,\n",
    "    \n",
    "  # WANDB LOGGING: \n",
    "  report_to = 'wandb',  # enable logging to W&B\n",
    "  run_name = 'ie1e-4-400w-50e-ovh-4-5-upgrade',   # Name your run, optional\n",
    "  load_best_model_at_end = True,  # This will ensure your best model will be uploaded to W&B\n",
    "  metric_for_best_model='wer',    # Load best model based on \"wer\", not eval loss\n",
    "  greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=common_voice_train,\n",
    "    eval_dataset=common_voice_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">ie1e-4-400w-50e-ovh-4-5-upgrade</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/wandb/xlsr-irish\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/wandb/xlsr-irish/runs/pr9wcve2\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish/runs/pr9wcve2</a><br/>\n",
       "                Run data is saved locally in <code>/workspace/wandb/run-20210323_082025-pr9wcve2</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1100' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1100/1100 2:11:17, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>15.702500</td>\n",
       "      <td>20.250538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.413300</td>\n",
       "      <td>10.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>15.459100</td>\n",
       "      <td>20.032469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.023600</td>\n",
       "      <td>10.322000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>16.054300</td>\n",
       "      <td>19.705692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.322400</td>\n",
       "      <td>10.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>16.154700</td>\n",
       "      <td>19.339045</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.047800</td>\n",
       "      <td>10.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>15.301700</td>\n",
       "      <td>18.895893</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.732800</td>\n",
       "      <td>10.174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>15.013600</td>\n",
       "      <td>18.419765</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.260600</td>\n",
       "      <td>10.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>9.856300</td>\n",
       "      <td>6.735880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.304300</td>\n",
       "      <td>10.263000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.896700</td>\n",
       "      <td>3.723217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.054900</td>\n",
       "      <td>10.109000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>3.293600</td>\n",
       "      <td>3.194918</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.492500</td>\n",
       "      <td>10.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.064100</td>\n",
       "      <td>3.075306</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.513800</td>\n",
       "      <td>10.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>3.053900</td>\n",
       "      <td>3.016027</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.887100</td>\n",
       "      <td>10.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.017100</td>\n",
       "      <td>2.949999</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.321700</td>\n",
       "      <td>10.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>2.968700</td>\n",
       "      <td>2.993623</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.696500</td>\n",
       "      <td>10.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.940800</td>\n",
       "      <td>2.940262</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.698900</td>\n",
       "      <td>10.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>2.938700</td>\n",
       "      <td>2.932228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>53.533900</td>\n",
       "      <td>9.452000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.912800</td>\n",
       "      <td>2.919930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>53.978800</td>\n",
       "      <td>9.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>2.920800</td>\n",
       "      <td>2.915820</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52.825800</td>\n",
       "      <td>9.579000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.890100</td>\n",
       "      <td>2.896046</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52.892400</td>\n",
       "      <td>9.567000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>2.884200</td>\n",
       "      <td>2.884246</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.757600</td>\n",
       "      <td>9.776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.913900</td>\n",
       "      <td>2.899337</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52.666300</td>\n",
       "      <td>9.608000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>2.822700</td>\n",
       "      <td>2.861765</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.817000</td>\n",
       "      <td>9.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.727800</td>\n",
       "      <td>2.717171</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52.385200</td>\n",
       "      <td>9.659000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>2.554500</td>\n",
       "      <td>2.467444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>54.229100</td>\n",
       "      <td>9.331000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.306500</td>\n",
       "      <td>2.168878</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52.478200</td>\n",
       "      <td>9.642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>1.911100</td>\n",
       "      <td>1.824011</td>\n",
       "      <td>0.987127</td>\n",
       "      <td>53.680600</td>\n",
       "      <td>9.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.686500</td>\n",
       "      <td>1.621776</td>\n",
       "      <td>1.022528</td>\n",
       "      <td>53.223700</td>\n",
       "      <td>9.507000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>1.409500</td>\n",
       "      <td>1.411929</td>\n",
       "      <td>0.981568</td>\n",
       "      <td>50.968900</td>\n",
       "      <td>9.928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.245800</td>\n",
       "      <td>1.306692</td>\n",
       "      <td>0.917203</td>\n",
       "      <td>53.715200</td>\n",
       "      <td>9.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>1.119200</td>\n",
       "      <td>1.237306</td>\n",
       "      <td>0.880925</td>\n",
       "      <td>52.299800</td>\n",
       "      <td>9.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.151800</td>\n",
       "      <td>1.196226</td>\n",
       "      <td>0.842891</td>\n",
       "      <td>50.379000</td>\n",
       "      <td>10.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.981400</td>\n",
       "      <td>1.130619</td>\n",
       "      <td>0.799590</td>\n",
       "      <td>49.970100</td>\n",
       "      <td>10.126000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.784900</td>\n",
       "      <td>1.123908</td>\n",
       "      <td>0.787888</td>\n",
       "      <td>49.915400</td>\n",
       "      <td>10.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.907100</td>\n",
       "      <td>1.096435</td>\n",
       "      <td>0.767408</td>\n",
       "      <td>50.258200</td>\n",
       "      <td>10.068000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>1.079041</td>\n",
       "      <td>0.766823</td>\n",
       "      <td>50.287200</td>\n",
       "      <td>10.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.704500</td>\n",
       "      <td>1.079502</td>\n",
       "      <td>0.758923</td>\n",
       "      <td>50.141400</td>\n",
       "      <td>10.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.848600</td>\n",
       "      <td>1.063557</td>\n",
       "      <td>0.757753</td>\n",
       "      <td>49.651300</td>\n",
       "      <td>10.191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.685500</td>\n",
       "      <td>1.065187</td>\n",
       "      <td>0.751609</td>\n",
       "      <td>50.129100</td>\n",
       "      <td>10.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.684200</td>\n",
       "      <td>1.065697</td>\n",
       "      <td>0.750146</td>\n",
       "      <td>50.317600</td>\n",
       "      <td>10.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.868400</td>\n",
       "      <td>1.054230</td>\n",
       "      <td>0.734933</td>\n",
       "      <td>49.905800</td>\n",
       "      <td>10.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.685200</td>\n",
       "      <td>1.056278</td>\n",
       "      <td>0.739029</td>\n",
       "      <td>50.433700</td>\n",
       "      <td>10.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.651600</td>\n",
       "      <td>1.060914</td>\n",
       "      <td>0.742832</td>\n",
       "      <td>50.221600</td>\n",
       "      <td>10.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.687600</td>\n",
       "      <td>1.047898</td>\n",
       "      <td>0.727326</td>\n",
       "      <td>50.464000</td>\n",
       "      <td>10.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.712700</td>\n",
       "      <td>1.043416</td>\n",
       "      <td>0.721182</td>\n",
       "      <td>51.303100</td>\n",
       "      <td>9.863000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.574400</td>\n",
       "      <td>1.045820</td>\n",
       "      <td>0.721767</td>\n",
       "      <td>50.493200</td>\n",
       "      <td>10.021000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 39995<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 1203.68MB of 1203.68MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/workspace/wandb/run-20210323_082025-pr9wcve2/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/workspace/wandb/run-20210323_082025-pr9wcve2/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.5744</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/epoch</td><td>50.0</td></tr><tr><td>_runtime</td><td>7883</td></tr><tr><td>_timestamp</td><td>1616495509</td></tr><tr><td>_step</td><td>1100</td></tr><tr><td>eval/loss</td><td>1.04582</td></tr><tr><td>eval/wer</td><td>0.72177</td></tr><tr><td>eval/runtime</td><td>50.4932</td></tr><tr><td>eval/samples_per_second</td><td>10.021</td></tr><tr><td>train/train_runtime</td><td>7883.305</td></tr><tr><td>train/train_samples_per_second</td><td>0.14</td></tr><tr><td>train/total_flos</td><td>7.283247395753042e+18</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▇█▇▆▆▅▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▂▂▃▃▃▄▅▅▆▆▆▇▇███▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>█████▇▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/wer</td><td>▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▆▅▄▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▃▁▁▂▂▁▁▂▃▂▃▂▂▇█▆▆▅▆▅█▆▇▇▄▇▅▃▂▂▃▃▂▂▃▂▃▃▃▃</td></tr><tr><td>eval/samples_per_second</td><td>▆██▇▇██▆▆▇▆▇▇▂▁▃▃▄▃▄▁▃▂▂▅▂▃▆▇▇▆▆▇▆▆▇▆▆▆▆</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">ie1e-4-400w-50e-ovh-4-5-upgrade</strong>: <a href=\"https://wandb.ai/wandb/xlsr-irish/runs/pr9wcve2\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish/runs/pr9wcve2</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-xlsr-53\", \n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.1,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.1,\n",
    "    gradient_checkpointing=True, \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=\"data\",\n",
    "  # output_dir=\"./wav2vec2-large-xlsr-turkish-demo\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=48,\n",
    "  per_device_eval_batch_size=64,\n",
    "  gradient_accumulation_steps=1,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=50,\n",
    "  fp16=True,\n",
    "  save_steps=25,\n",
    "  eval_steps=25,\n",
    "  logging_steps=5,\n",
    "  learning_rate=1e-4,\n",
    "  warmup_steps=100,\n",
    "  save_total_limit=1,\n",
    "    \n",
    "  # WANDB LOGGING: \n",
    "  report_to = 'wandb',  # enable logging to W&B\n",
    "  run_name = 'ie1e-4-100w-d-o',   # Name your run, optional\n",
    "  load_best_model_at_end = True,  # This will ensure your best model will be uploaded to W&B\n",
    "  metric_for_best_model='wer',    # Load best model based on \"wer\", not eval loss\n",
    "  greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=common_voice_train,\n",
    "    eval_dataset=common_voice_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">ie1e-4-100w-d-o</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/wandb/xlsr-irish\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/wandb/xlsr-irish/runs/betqqchv\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish/runs/betqqchv</a><br/>\n",
       "                Run data is saved locally in <code>/workspace/wandb/run-20210323_105809-betqqchv</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1100' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1100/1100 2:07:11, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>15.554500</td>\n",
       "      <td>19.991453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.343500</td>\n",
       "      <td>9.855000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>19.053507</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.464000</td>\n",
       "      <td>10.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>13.980400</td>\n",
       "      <td>17.558977</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.705100</td>\n",
       "      <td>9.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>8.818000</td>\n",
       "      <td>4.919689</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.143100</td>\n",
       "      <td>10.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>3.298300</td>\n",
       "      <td>3.217547</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.669200</td>\n",
       "      <td>9.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.081700</td>\n",
       "      <td>3.042261</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.392100</td>\n",
       "      <td>10.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>3.028000</td>\n",
       "      <td>3.013988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.371200</td>\n",
       "      <td>10.045000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.980600</td>\n",
       "      <td>3.010421</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.736800</td>\n",
       "      <td>9.973000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>3.003500</td>\n",
       "      <td>2.996748</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.841000</td>\n",
       "      <td>9.761000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.933300</td>\n",
       "      <td>2.951648</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.574900</td>\n",
       "      <td>10.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.945300</td>\n",
       "      <td>2.941489</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52.292600</td>\n",
       "      <td>9.676000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.987800</td>\n",
       "      <td>2.935616</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.705100</td>\n",
       "      <td>9.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>2.911000</td>\n",
       "      <td>2.919569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.889100</td>\n",
       "      <td>9.943000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>2.917126</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.649500</td>\n",
       "      <td>9.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>2.913700</td>\n",
       "      <td>2.914048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.028200</td>\n",
       "      <td>9.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.899700</td>\n",
       "      <td>2.904325</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.767300</td>\n",
       "      <td>9.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>2.908800</td>\n",
       "      <td>2.896867</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.809600</td>\n",
       "      <td>9.767000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.896300</td>\n",
       "      <td>2.903282</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.502000</td>\n",
       "      <td>10.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>2.908000</td>\n",
       "      <td>2.893250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.926500</td>\n",
       "      <td>10.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.885900</td>\n",
       "      <td>2.895417</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.275000</td>\n",
       "      <td>10.065000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>2.886200</td>\n",
       "      <td>2.900954</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.345400</td>\n",
       "      <td>10.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.875500</td>\n",
       "      <td>2.885917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.852700</td>\n",
       "      <td>9.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>2.877500</td>\n",
       "      <td>2.871844</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.598200</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.877200</td>\n",
       "      <td>2.865247</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.540300</td>\n",
       "      <td>9.818000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>2.850900</td>\n",
       "      <td>2.848702</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.243200</td>\n",
       "      <td>9.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.834700</td>\n",
       "      <td>2.821357</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.157100</td>\n",
       "      <td>9.891000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>2.756300</td>\n",
       "      <td>2.725611</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.221900</td>\n",
       "      <td>9.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.655800</td>\n",
       "      <td>2.606270</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.108500</td>\n",
       "      <td>9.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>2.532700</td>\n",
       "      <td>2.466610</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.578100</td>\n",
       "      <td>10.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.477000</td>\n",
       "      <td>2.326723</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.991300</td>\n",
       "      <td>9.732000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>2.323400</td>\n",
       "      <td>2.170647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.441300</td>\n",
       "      <td>9.836000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.132800</td>\n",
       "      <td>2.042260</td>\n",
       "      <td>0.996489</td>\n",
       "      <td>52.230900</td>\n",
       "      <td>9.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>2.086700</td>\n",
       "      <td>1.873823</td>\n",
       "      <td>0.981276</td>\n",
       "      <td>51.983400</td>\n",
       "      <td>9.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.873400</td>\n",
       "      <td>1.722767</td>\n",
       "      <td>0.941486</td>\n",
       "      <td>51.182200</td>\n",
       "      <td>9.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>1.729900</td>\n",
       "      <td>1.611290</td>\n",
       "      <td>0.895553</td>\n",
       "      <td>50.736100</td>\n",
       "      <td>9.973000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.726900</td>\n",
       "      <td>1.516195</td>\n",
       "      <td>0.895553</td>\n",
       "      <td>51.464700</td>\n",
       "      <td>9.832000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>1.548400</td>\n",
       "      <td>1.470073</td>\n",
       "      <td>0.926858</td>\n",
       "      <td>51.048600</td>\n",
       "      <td>9.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.548700</td>\n",
       "      <td>1.422159</td>\n",
       "      <td>0.908426</td>\n",
       "      <td>51.624800</td>\n",
       "      <td>9.801000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>1.623800</td>\n",
       "      <td>1.388104</td>\n",
       "      <td>0.908719</td>\n",
       "      <td>51.617600</td>\n",
       "      <td>9.803000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.460400</td>\n",
       "      <td>1.361929</td>\n",
       "      <td>0.895553</td>\n",
       "      <td>51.478800</td>\n",
       "      <td>9.829000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>1.393400</td>\n",
       "      <td>1.341388</td>\n",
       "      <td>0.885020</td>\n",
       "      <td>51.404000</td>\n",
       "      <td>9.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.431700</td>\n",
       "      <td>1.325718</td>\n",
       "      <td>0.880925</td>\n",
       "      <td>52.131600</td>\n",
       "      <td>9.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>1.499500</td>\n",
       "      <td>1.323907</td>\n",
       "      <td>0.883850</td>\n",
       "      <td>51.787600</td>\n",
       "      <td>9.771000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.374800</td>\n",
       "      <td>1.320893</td>\n",
       "      <td>0.879462</td>\n",
       "      <td>53.034400</td>\n",
       "      <td>9.541000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 41683<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 1203.68MB of 1203.68MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/workspace/wandb/run-20210323_105809-betqqchv/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/workspace/wandb/run-20210323_105809-betqqchv/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>1.3748</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/epoch</td><td>50.0</td></tr><tr><td>_runtime</td><td>7635</td></tr><tr><td>_timestamp</td><td>1616504724</td></tr><tr><td>_step</td><td>1100</td></tr><tr><td>eval/loss</td><td>1.32089</td></tr><tr><td>eval/wer</td><td>0.87946</td></tr><tr><td>eval/runtime</td><td>53.0344</td></tr><tr><td>eval/samples_per_second</td><td>9.541</td></tr><tr><td>train/train_runtime</td><td>7635.9547</td></tr><tr><td>train/train_samples_per_second</td><td>0.144</td></tr><tr><td>train/total_flos</td><td>7.283247395753042e+18</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▇▇▅▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>██▇▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/wer</td><td>██████████████████████████████▅▂▂▄▃▃▂▁▁▁</td></tr><tr><td>eval/runtime</td><td>▄▂▃▁▃▂▂▃▅▂▃▃▅▃▅▅▂▁▂▂▃▅▄▄▄▄▂▆▄▆▄▃▄▄▅▅▄▄▆█</td></tr><tr><td>eval/samples_per_second</td><td>▅▇▆▇▆▇▇▆▄▆▆▆▄▅▄▄▇█▇▇▆▄▅▅▅▅▆▃▄▃▅▆▄▅▄▄▄▅▃▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">ie1e-4-100w-d-o</strong>: <a href=\"https://wandb.ai/wandb/xlsr-irish/runs/betqqchv\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish/runs/betqqchv</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove \"(,),-\" and drop lr ro 3e-5 and pad to multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-xlsr-53\", \n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.1,\n",
    "    gradient_checkpointing=True, \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=\"data\",\n",
    "  # output_dir=\"./wav2vec2-large-xlsr-turkish-demo\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=48,\n",
    "  per_device_eval_batch_size=64,\n",
    "  gradient_accumulation_steps=1,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=50,\n",
    "  fp16=True,\n",
    "  save_steps=25,\n",
    "  eval_steps=25,\n",
    "  logging_steps=5,\n",
    "  learning_rate=3e-5,\n",
    "  warmup_steps=200,\n",
    "  save_total_limit=1,\n",
    "    \n",
    "  # WANDB LOGGING: \n",
    "  report_to = 'wandb',  # enable logging to W&B\n",
    "  run_name = 'lr-3e5-200w',   # Name your run, optional\n",
    "  load_best_model_at_end = True,  # This will ensure your best model will be uploaded to W&B\n",
    "  metric_for_best_model='wer',    # Load best model based on \"wer\", not eval loss\n",
    "  greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=common_voice_train,\n",
    "    eval_dataset=common_voice_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">lr-3e5-200w</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/wandb/xlsr-irish\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/wandb/xlsr-irish/runs/280jgmkw\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish/runs/280jgmkw</a><br/>\n",
       "                Run data is saved locally in <code>/workspace/wandb/run-20210323_135656-280jgmkw</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1100' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1100/1100 2:08:11, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>15.151200</td>\n",
       "      <td>19.472984</td>\n",
       "      <td>0.999122</td>\n",
       "      <td>51.240400</td>\n",
       "      <td>9.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>15.216700</td>\n",
       "      <td>19.326269</td>\n",
       "      <td>0.995319</td>\n",
       "      <td>50.926700</td>\n",
       "      <td>9.936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>15.852400</td>\n",
       "      <td>19.085432</td>\n",
       "      <td>0.996782</td>\n",
       "      <td>52.112000</td>\n",
       "      <td>9.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>16.335200</td>\n",
       "      <td>18.752697</td>\n",
       "      <td>0.997367</td>\n",
       "      <td>51.335900</td>\n",
       "      <td>9.857000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>15.900900</td>\n",
       "      <td>18.325800</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.482800</td>\n",
       "      <td>10.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>14.765500</td>\n",
       "      <td>17.670265</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.488400</td>\n",
       "      <td>10.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>14.375800</td>\n",
       "      <td>16.893814</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.254700</td>\n",
       "      <td>10.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>12.220900</td>\n",
       "      <td>16.022058</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.918100</td>\n",
       "      <td>9.938000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>9.895600</td>\n",
       "      <td>11.388740</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.557300</td>\n",
       "      <td>10.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.874900</td>\n",
       "      <td>3.987979</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.239200</td>\n",
       "      <td>10.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>4.325700</td>\n",
       "      <td>3.441367</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.197000</td>\n",
       "      <td>10.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.329800</td>\n",
       "      <td>3.211843</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.404300</td>\n",
       "      <td>10.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>3.246400</td>\n",
       "      <td>3.124217</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.475700</td>\n",
       "      <td>10.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.099600</td>\n",
       "      <td>3.078398</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.452000</td>\n",
       "      <td>10.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>3.078300</td>\n",
       "      <td>3.055323</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.183000</td>\n",
       "      <td>10.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.030800</td>\n",
       "      <td>3.036931</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.221400</td>\n",
       "      <td>10.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>3.051400</td>\n",
       "      <td>3.031436</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>51.716700</td>\n",
       "      <td>9.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.014300</td>\n",
       "      <td>3.014094</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.743600</td>\n",
       "      <td>9.972000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>2.999600</td>\n",
       "      <td>2.986253</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.469500</td>\n",
       "      <td>10.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.952000</td>\n",
       "      <td>2.967622</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>51.197700</td>\n",
       "      <td>9.883000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>2.952500</td>\n",
       "      <td>2.953533</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>51.149400</td>\n",
       "      <td>9.893000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.942400</td>\n",
       "      <td>2.943630</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.757800</td>\n",
       "      <td>9.969000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>2.940700</td>\n",
       "      <td>2.939342</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.807800</td>\n",
       "      <td>9.959000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.946100</td>\n",
       "      <td>2.954757</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>51.028100</td>\n",
       "      <td>9.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>2.927700</td>\n",
       "      <td>2.930294</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>53.291800</td>\n",
       "      <td>9.495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.939300</td>\n",
       "      <td>2.926994</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>51.558300</td>\n",
       "      <td>9.814000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>2.922000</td>\n",
       "      <td>2.928046</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>88.308800</td>\n",
       "      <td>5.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.909500</td>\n",
       "      <td>2.920527</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>51.187300</td>\n",
       "      <td>9.885000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>2.895600</td>\n",
       "      <td>2.918448</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.602300</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.917600</td>\n",
       "      <td>2.916227</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>51.279400</td>\n",
       "      <td>9.868000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>2.926900</td>\n",
       "      <td>2.919862</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.651700</td>\n",
       "      <td>9.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.913800</td>\n",
       "      <td>2.912603</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>51.136500</td>\n",
       "      <td>9.895000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>2.929500</td>\n",
       "      <td>2.918930</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>51.098200</td>\n",
       "      <td>9.903000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.910200</td>\n",
       "      <td>2.912631</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.702200</td>\n",
       "      <td>9.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>2.916200</td>\n",
       "      <td>2.912000</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>51.176000</td>\n",
       "      <td>9.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.915100</td>\n",
       "      <td>2.910292</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>51.076200</td>\n",
       "      <td>9.907000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>2.898200</td>\n",
       "      <td>2.909858</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>51.605300</td>\n",
       "      <td>9.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.908600</td>\n",
       "      <td>2.910465</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>52.138100</td>\n",
       "      <td>9.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>2.902300</td>\n",
       "      <td>2.907041</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>51.666500</td>\n",
       "      <td>9.794000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.907200</td>\n",
       "      <td>2.905974</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>51.579600</td>\n",
       "      <td>9.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>2.906900</td>\n",
       "      <td>2.905101</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>51.555200</td>\n",
       "      <td>9.815000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.913100</td>\n",
       "      <td>2.904213</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>51.201900</td>\n",
       "      <td>9.882000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>2.911900</td>\n",
       "      <td>2.903719</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>51.982200</td>\n",
       "      <td>9.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.914400</td>\n",
       "      <td>2.904411</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>50.917700</td>\n",
       "      <td>9.938000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 51338<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 1203.67MB of 1203.67MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/workspace/wandb/run-20210323_135656-280jgmkw/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/workspace/wandb/run-20210323_135656-280jgmkw/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>2.9144</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/epoch</td><td>50.0</td></tr><tr><td>_runtime</td><td>7696</td></tr><tr><td>_timestamp</td><td>1616515512</td></tr><tr><td>_step</td><td>1100</td></tr><tr><td>eval/loss</td><td>2.90441</td></tr><tr><td>eval/wer</td><td>0.99971</td></tr><tr><td>eval/runtime</td><td>50.9177</td></tr><tr><td>eval/samples_per_second</td><td>9.938</td></tr><tr><td>train/train_runtime</td><td>7696.2249</td></tr><tr><td>train/train_samples_per_second</td><td>0.143</td></tr><tr><td>train/total_flos</td><td>7.283200068608901e+18</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▇█▇▇▇▆▆▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▂▃▄▅▆▇███▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>█████▇▇▇▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/wer</td><td>▇▁▃▄████████████████████████████████████</td></tr><tr><td>eval/runtime</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>██▇███████████████████▇█▁█████████▇█████</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">lr-3e5-200w</strong>: <a href=\"https://wandb.ai/wandb/xlsr-irish/runs/280jgmkw\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish/runs/280jgmkw</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With LoudNorm, 3e-4, 200w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_voice_train = common_voice_train.map(prepare_dataset, remove_columns=common_voice_train.column_names, batch_size=8, num_proc=4, batched=True)\n",
    "# common_voice_test = common_voice_test.map(prepare_dataset, remove_columns=common_voice_test.column_names, batch_size=8, num_proc=4, batched=True)\n",
    "\n",
    "# data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True,\n",
    "#                                           pad_to_multiple_of=8, pad_to_multiple_of_labels=8)\n",
    "\n",
    "# wer_metric = load_metric(\"wer\")\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#     pred_logits = pred.predictions\n",
    "#     pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "#     pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "#     pred_str = processor.batch_decode(pred_ids)\n",
    "#     # we do not want to group tokens when computing the metrics\n",
    "#     label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "#     wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "#     return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-xlsr-53\", \n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.1,\n",
    "    gradient_checkpointing=True, \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=\"data\",\n",
    "  # output_dir=\"./wav2vec2-large-xlsr-turkish-demo\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=48,\n",
    "  per_device_eval_batch_size=64,\n",
    "  gradient_accumulation_steps=1,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=50,\n",
    "  fp16=True,\n",
    "  save_steps=25,\n",
    "  eval_steps=25,\n",
    "  logging_steps=5,\n",
    "  learning_rate=3e-4,\n",
    "  warmup_steps=200,\n",
    "  save_total_limit=1,\n",
    "    \n",
    "  # WANDB LOGGING: \n",
    "  report_to = 'wandb',  # enable logging to W&B\n",
    "  run_name = 'ie3e-4-200w',   # Name your run, optional\n",
    "  load_best_model_at_end = True,  # This will ensure your best model will be uploaded to W&B\n",
    "  metric_for_best_model='wer',    # Load best model based on \"wer\", not eval loss\n",
    "  greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=common_voice_train,\n",
    "    eval_dataset=common_voice_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">ie3e-4-200w-loud_norm</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/wandb/xlsr-irish\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/wandb/xlsr-irish/runs/1ntax96u\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish/runs/1ntax96u</a><br/>\n",
       "                Run data is saved locally in <code>/workspace/wandb/run-20210323_163351-1ntax96u</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1100' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1100/1100 2:06:42, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>15.277300</td>\n",
       "      <td>19.644337</td>\n",
       "      <td>2.052077</td>\n",
       "      <td>50.997200</td>\n",
       "      <td>9.922000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>13.118300</td>\n",
       "      <td>18.081715</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.547300</td>\n",
       "      <td>10.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>12.811100</td>\n",
       "      <td>16.340801</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.731000</td>\n",
       "      <td>10.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.615500</td>\n",
       "      <td>3.323051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.162800</td>\n",
       "      <td>10.087000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>3.061500</td>\n",
       "      <td>3.024457</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>54.569900</td>\n",
       "      <td>9.273000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.020800</td>\n",
       "      <td>3.020138</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.498400</td>\n",
       "      <td>10.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>2.999400</td>\n",
       "      <td>2.983362</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.936600</td>\n",
       "      <td>10.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.021000</td>\n",
       "      <td>3.006591</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.037100</td>\n",
       "      <td>10.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>2.980700</td>\n",
       "      <td>2.983737</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.545400</td>\n",
       "      <td>10.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.928100</td>\n",
       "      <td>2.924066</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.037200</td>\n",
       "      <td>10.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.924900</td>\n",
       "      <td>2.915181</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.257700</td>\n",
       "      <td>10.068000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.908700</td>\n",
       "      <td>2.971997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.059300</td>\n",
       "      <td>10.108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>2.888000</td>\n",
       "      <td>2.954283</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.785100</td>\n",
       "      <td>9.964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.893500</td>\n",
       "      <td>2.907286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.141200</td>\n",
       "      <td>9.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>2.897100</td>\n",
       "      <td>2.889808</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.007000</td>\n",
       "      <td>10.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.897100</td>\n",
       "      <td>2.897569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.647000</td>\n",
       "      <td>9.991000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>2.899200</td>\n",
       "      <td>2.887484</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.013300</td>\n",
       "      <td>10.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.880600</td>\n",
       "      <td>2.876290</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.392200</td>\n",
       "      <td>10.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>2.848700</td>\n",
       "      <td>2.841546</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.140400</td>\n",
       "      <td>10.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.736700</td>\n",
       "      <td>2.640816</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.538600</td>\n",
       "      <td>10.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>2.488200</td>\n",
       "      <td>2.380572</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.847000</td>\n",
       "      <td>9.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.963800</td>\n",
       "      <td>1.807132</td>\n",
       "      <td>0.947338</td>\n",
       "      <td>50.317000</td>\n",
       "      <td>10.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>1.462700</td>\n",
       "      <td>1.400030</td>\n",
       "      <td>0.943534</td>\n",
       "      <td>50.900300</td>\n",
       "      <td>9.941000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.211100</td>\n",
       "      <td>1.307465</td>\n",
       "      <td>0.852545</td>\n",
       "      <td>50.601000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>1.181947</td>\n",
       "      <td>0.816852</td>\n",
       "      <td>50.770500</td>\n",
       "      <td>9.966000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.828800</td>\n",
       "      <td>1.149050</td>\n",
       "      <td>0.789643</td>\n",
       "      <td>50.810500</td>\n",
       "      <td>9.959000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.678500</td>\n",
       "      <td>1.074444</td>\n",
       "      <td>0.784084</td>\n",
       "      <td>51.437200</td>\n",
       "      <td>9.837000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.573400</td>\n",
       "      <td>1.050706</td>\n",
       "      <td>0.734055</td>\n",
       "      <td>50.948200</td>\n",
       "      <td>9.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.503600</td>\n",
       "      <td>1.036522</td>\n",
       "      <td>0.694266</td>\n",
       "      <td>51.288200</td>\n",
       "      <td>9.866000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.546500</td>\n",
       "      <td>1.074543</td>\n",
       "      <td>0.709772</td>\n",
       "      <td>52.203100</td>\n",
       "      <td>9.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.408300</td>\n",
       "      <td>1.031528</td>\n",
       "      <td>0.683733</td>\n",
       "      <td>51.558200</td>\n",
       "      <td>9.814000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.269600</td>\n",
       "      <td>1.081162</td>\n",
       "      <td>0.689877</td>\n",
       "      <td>51.064400</td>\n",
       "      <td>9.909000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.406800</td>\n",
       "      <td>1.050526</td>\n",
       "      <td>0.679052</td>\n",
       "      <td>51.467100</td>\n",
       "      <td>9.832000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.282400</td>\n",
       "      <td>1.050074</td>\n",
       "      <td>0.677589</td>\n",
       "      <td>50.863100</td>\n",
       "      <td>9.948000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.241700</td>\n",
       "      <td>1.067466</td>\n",
       "      <td>0.670275</td>\n",
       "      <td>51.139100</td>\n",
       "      <td>9.895000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.376200</td>\n",
       "      <td>1.066746</td>\n",
       "      <td>0.648625</td>\n",
       "      <td>51.218500</td>\n",
       "      <td>9.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.239700</td>\n",
       "      <td>1.085992</td>\n",
       "      <td>0.663546</td>\n",
       "      <td>51.568900</td>\n",
       "      <td>9.812000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.223000</td>\n",
       "      <td>1.100686</td>\n",
       "      <td>0.656817</td>\n",
       "      <td>51.032900</td>\n",
       "      <td>9.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>1.094568</td>\n",
       "      <td>0.646870</td>\n",
       "      <td>51.532400</td>\n",
       "      <td>9.819000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.255200</td>\n",
       "      <td>1.114089</td>\n",
       "      <td>0.654184</td>\n",
       "      <td>51.852700</td>\n",
       "      <td>9.758000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.243100</td>\n",
       "      <td>1.114330</td>\n",
       "      <td>0.650673</td>\n",
       "      <td>51.149100</td>\n",
       "      <td>9.893000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.236300</td>\n",
       "      <td>1.107052</td>\n",
       "      <td>0.645407</td>\n",
       "      <td>51.622100</td>\n",
       "      <td>9.802000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.256800</td>\n",
       "      <td>1.112712</td>\n",
       "      <td>0.644529</td>\n",
       "      <td>51.414400</td>\n",
       "      <td>9.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>1.110060</td>\n",
       "      <td>0.638970</td>\n",
       "      <td>51.637800</td>\n",
       "      <td>9.799000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5246<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 1203.67MB of 1203.67MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/workspace/wandb/run-20210323_163351-1ntax96u/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/workspace/wandb/run-20210323_163351-1ntax96u/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.1639</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/epoch</td><td>50.0</td></tr><tr><td>_runtime</td><td>7606</td></tr><tr><td>_timestamp</td><td>1616524838</td></tr><tr><td>_step</td><td>1100</td></tr><tr><td>eval/loss</td><td>1.11006</td></tr><tr><td>eval/wer</td><td>0.63897</td></tr><tr><td>eval/runtime</td><td>51.6378</td></tr><tr><td>eval/samples_per_second</td><td>9.799</td></tr><tr><td>train/train_runtime</td><td>7606.5778</td></tr><tr><td>train/train_samples_per_second</td><td>0.145</td></tr><tr><td>train/total_flos</td><td>7.283200068608901e+18</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▇▇▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▂▃▄▅▆▇███▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>█▇▇▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/wer</td><td>█▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▃▁▁▂█▂▂▂▁▂▂▃▃▂▃▂▂▂▂▃▃▂▃▃▄▃▃▅▄▃▃▃▃▄▃▄▄▃▄▄</td></tr><tr><td>eval/samples_per_second</td><td>▆██▇▁▇▇▇█▇▇▆▆▇▆▇▇▇▇▆▆▆▆▆▅▆▅▄▅▆▆▆▆▅▆▅▅▆▅▅</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">ie3e-4-200w-loud_norm</strong>: <a href=\"https://wandb.ai/wandb/xlsr-irish/runs/1ntax96u\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish/runs/1ntax96u</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR SCHEDULE 10-40-50, LoudNorm, 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lr_lambda(current_step):\n",
    "#     if current_step < warmup_steps:\n",
    "#         return float(current_step) / float(max(1, warmup_steps))\n",
    "#     elif current_step < warmup_steps+constant_steps:\n",
    "#         return 1\n",
    "#     else: max(\n",
    "#         0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - (warmup_steps+constant_steps)))\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Optimizer, AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from transformers.trainer_utils import SchedulerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flat_linear_schedule_with_warmup(optimizer:Optimizer, num_warmup_steps:int,\n",
    "                                         num_training_steps:int, last_epoch:int =-1):\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        constant_steps = int(num_training_steps * 0.4)\n",
    "        warmup_steps = int(num_training_steps * 0.1)\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        elif current_step < warmup_steps+constant_steps:\n",
    "            return 1\n",
    "        else: return max(\n",
    "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - (warmup_steps+constant_steps)))\n",
    "        )\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "\n",
    "def get_flat_cheduler(\n",
    "    name: Union[str, SchedulerType] = None,\n",
    "    optimizer: Optimizer = None,\n",
    "    num_warmup_steps: Optional[int] = None,\n",
    "    num_training_steps: Optional[int] = None,\n",
    "):\n",
    "    return get_flat_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, \n",
    "                                                num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "     \n",
    "    def create_flat_scheduler(self, num_training_steps: int):\n",
    "        self.lr_scheduler = get_flat_cheduler(optimizer = self.optimizer,\n",
    "                                              num_training_steps=num_training_steps)\n",
    "    \n",
    "    def create_optimizer_and_scheduler(self, num_training_steps):\n",
    "        self.create_optimizer()\n",
    "        self.create_flat_scheduler(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-xlsr-53\", \n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.1,\n",
    "    gradient_checkpointing=True, \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    ctc_zero_infinity = True,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=\"data\",\n",
    "  # output_dir=\"./wav2vec2-large-xlsr-turkish-demo\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=48,\n",
    "  per_device_eval_batch_size=64,\n",
    "  gradient_accumulation_steps=1,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=50,\n",
    "  fp16=True,\n",
    "  save_steps=25,\n",
    "  eval_steps=25,\n",
    "  logging_steps=5,\n",
    "  learning_rate=3e-4,\n",
    "  warmup_steps=200,\n",
    "  save_total_limit=1,\n",
    "    \n",
    "  # WANDB LOGGING: \n",
    "  report_to = 'wandb',  # enable logging to W&B\n",
    "  run_name = 'flat_linear_lr-3e4',   # Name your run, optional\n",
    "  load_best_model_at_end = True,  # This will ensure your best model will be uploaded to W&B\n",
    "  metric_for_best_model='wer',    # Load best model based on \"wer\", not eval loss\n",
    "  greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = FlatTrainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=common_voice_train,\n",
    "    eval_dataset=common_voice_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">flat_linear_lr-3e4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/wandb/xlsr-irish\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/wandb/xlsr-irish/runs/2u22bqdc\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish/runs/2u22bqdc</a><br/>\n",
       "                Run data is saved locally in <code>/workspace/wandb/run-20210323_213419-2u22bqdc</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1100' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1100/1100 2:05:56, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>14.874300</td>\n",
       "      <td>18.996742</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.487700</td>\n",
       "      <td>10.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>11.763400</td>\n",
       "      <td>15.761393</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.282600</td>\n",
       "      <td>10.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>3.771500</td>\n",
       "      <td>3.328754</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.717100</td>\n",
       "      <td>10.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.076200</td>\n",
       "      <td>3.019775</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.695800</td>\n",
       "      <td>10.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>3.027500</td>\n",
       "      <td>2.980913</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.733500</td>\n",
       "      <td>10.174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.949900</td>\n",
       "      <td>2.934928</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.129100</td>\n",
       "      <td>10.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>2.963200</td>\n",
       "      <td>2.998456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.870300</td>\n",
       "      <td>10.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.912000</td>\n",
       "      <td>2.910007</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.957000</td>\n",
       "      <td>10.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>2.944400</td>\n",
       "      <td>3.083307</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.625800</td>\n",
       "      <td>10.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.900300</td>\n",
       "      <td>2.913093</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.844600</td>\n",
       "      <td>10.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.924700</td>\n",
       "      <td>2.929119</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.556600</td>\n",
       "      <td>10.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.899800</td>\n",
       "      <td>2.925853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.236300</td>\n",
       "      <td>10.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>2.890100</td>\n",
       "      <td>2.903414</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.811600</td>\n",
       "      <td>10.158000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.886300</td>\n",
       "      <td>2.878301</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.464600</td>\n",
       "      <td>10.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>2.884200</td>\n",
       "      <td>2.878740</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.964400</td>\n",
       "      <td>10.127000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.883900</td>\n",
       "      <td>2.923744</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.541200</td>\n",
       "      <td>10.214000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>2.874900</td>\n",
       "      <td>2.857235</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.988700</td>\n",
       "      <td>10.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.774600</td>\n",
       "      <td>2.760185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.791900</td>\n",
       "      <td>10.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>2.688900</td>\n",
       "      <td>2.595747</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.183900</td>\n",
       "      <td>10.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.295300</td>\n",
       "      <td>2.185716</td>\n",
       "      <td>0.979520</td>\n",
       "      <td>50.648500</td>\n",
       "      <td>9.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>1.656600</td>\n",
       "      <td>1.686133</td>\n",
       "      <td>0.989175</td>\n",
       "      <td>50.196100</td>\n",
       "      <td>10.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.384900</td>\n",
       "      <td>1.446234</td>\n",
       "      <td>0.976009</td>\n",
       "      <td>50.019500</td>\n",
       "      <td>10.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>1.113000</td>\n",
       "      <td>1.284735</td>\n",
       "      <td>0.897308</td>\n",
       "      <td>49.972700</td>\n",
       "      <td>10.126000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.934800</td>\n",
       "      <td>1.185111</td>\n",
       "      <td>0.827092</td>\n",
       "      <td>49.658700</td>\n",
       "      <td>10.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.696500</td>\n",
       "      <td>1.161604</td>\n",
       "      <td>0.799298</td>\n",
       "      <td>50.382400</td>\n",
       "      <td>10.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.649700</td>\n",
       "      <td>1.191859</td>\n",
       "      <td>0.813926</td>\n",
       "      <td>50.447400</td>\n",
       "      <td>10.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.533900</td>\n",
       "      <td>1.189476</td>\n",
       "      <td>0.804857</td>\n",
       "      <td>49.945100</td>\n",
       "      <td>10.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.449300</td>\n",
       "      <td>1.219489</td>\n",
       "      <td>0.789936</td>\n",
       "      <td>50.687300</td>\n",
       "      <td>9.983000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.383700</td>\n",
       "      <td>1.225473</td>\n",
       "      <td>0.773259</td>\n",
       "      <td>49.725700</td>\n",
       "      <td>10.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.405600</td>\n",
       "      <td>1.267739</td>\n",
       "      <td>0.769748</td>\n",
       "      <td>50.603000</td>\n",
       "      <td>9.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.318700</td>\n",
       "      <td>1.232562</td>\n",
       "      <td>0.740492</td>\n",
       "      <td>50.758100</td>\n",
       "      <td>9.969000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.183300</td>\n",
       "      <td>1.270341</td>\n",
       "      <td>0.729959</td>\n",
       "      <td>50.818700</td>\n",
       "      <td>9.957000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.287598</td>\n",
       "      <td>0.724693</td>\n",
       "      <td>50.403500</td>\n",
       "      <td>10.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.232100</td>\n",
       "      <td>1.309324</td>\n",
       "      <td>0.737273</td>\n",
       "      <td>50.155900</td>\n",
       "      <td>10.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>1.293364</td>\n",
       "      <td>0.719427</td>\n",
       "      <td>50.965200</td>\n",
       "      <td>9.928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.298100</td>\n",
       "      <td>1.337456</td>\n",
       "      <td>0.701287</td>\n",
       "      <td>50.706500</td>\n",
       "      <td>9.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.179400</td>\n",
       "      <td>1.346216</td>\n",
       "      <td>0.721182</td>\n",
       "      <td>51.592100</td>\n",
       "      <td>9.808000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.146100</td>\n",
       "      <td>1.394526</td>\n",
       "      <td>0.722937</td>\n",
       "      <td>51.207800</td>\n",
       "      <td>9.881000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.381300</td>\n",
       "      <td>1.347878</td>\n",
       "      <td>0.708016</td>\n",
       "      <td>50.583600</td>\n",
       "      <td>10.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.160200</td>\n",
       "      <td>1.348498</td>\n",
       "      <td>0.709187</td>\n",
       "      <td>50.341600</td>\n",
       "      <td>10.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.177200</td>\n",
       "      <td>1.359920</td>\n",
       "      <td>0.708016</td>\n",
       "      <td>50.957300</td>\n",
       "      <td>9.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>1.342722</td>\n",
       "      <td>0.706846</td>\n",
       "      <td>51.444600</td>\n",
       "      <td>9.836000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.153900</td>\n",
       "      <td>1.360508</td>\n",
       "      <td>0.701287</td>\n",
       "      <td>50.686400</td>\n",
       "      <td>9.983000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.356772</td>\n",
       "      <td>0.700117</td>\n",
       "      <td>50.694300</td>\n",
       "      <td>9.981000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10332<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 1203.67MB of 1203.67MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/workspace/wandb/run-20210323_213419-2u22bqdc/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/workspace/wandb/run-20210323_213419-2u22bqdc/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.1</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/epoch</td><td>50.0</td></tr><tr><td>_runtime</td><td>7561</td></tr><tr><td>_timestamp</td><td>1616542820</td></tr><tr><td>_step</td><td>1100</td></tr><tr><td>eval/loss</td><td>1.35677</td></tr><tr><td>eval/wer</td><td>0.70012</td></tr><tr><td>eval/runtime</td><td>50.6943</td></tr><tr><td>eval/samples_per_second</td><td>9.981</td></tr><tr><td>train/train_runtime</td><td>7561.1099</td></tr><tr><td>train/train_samples_per_second</td><td>0.145</td></tr><tr><td>train/total_flos</td><td>7.283200068608901e+18</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▇▅▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▂▃▅▇█████████████████▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>█▇▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/wer</td><td>████████████████████▆▄▃▄▃▃▃▃▂▂▂▁▁▁▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▄▄▂▂▂▃▂▂▁▂▃▂▄▂▁▃▂▃▅▃▂▁▄▄▂▅▂▅▅▅▃▆▅█▇▅▄▆▇▅</td></tr><tr><td>eval/samples_per_second</td><td>▅▅▇▇▇▆▇▇█▇▆▇▅▇█▆▇▆▄▆▆█▅▅▇▄▇▄▄▄▆▃▄▁▂▄▅▃▁▄</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">flat_linear_lr-3e4</strong>: <a href=\"https://wandb.ai/wandb/xlsr-irish/runs/2u22bqdc\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish/runs/2u22bqdc</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loud Norm 100e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-xlsr-53\", \n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.1,\n",
    "    gradient_checkpointing=True, \n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    ctc_zero_infinity = True,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=\"data\",\n",
    "  # output_dir=\"./wav2vec2-large-xlsr-turkish-demo\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=48,\n",
    "  per_device_eval_batch_size=64,\n",
    "  gradient_accumulation_steps=1,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=100,\n",
    "  fp16=True,\n",
    "  save_steps=25,\n",
    "  eval_steps=25,\n",
    "  logging_steps=5,\n",
    "  learning_rate=3e-4,\n",
    "  warmup_steps=200,\n",
    "  save_total_limit=1,\n",
    "    \n",
    "  # WANDB LOGGING: \n",
    "  report_to = 'wandb',  # enable logging to W&B\n",
    "  run_name = 'ie3e-4-200w',   # Name your run, optional\n",
    "  load_best_model_at_end = True,  # This will ensure your best model will be uploaded to W&B\n",
    "  metric_for_best_model='wer',    # Load best model based on \"wer\", not eval loss\n",
    "  greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=common_voice_train,\n",
    "    eval_dataset=common_voice_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">ie3e-4-200w</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/wandb/xlsr-irish\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/wandb/xlsr-irish/runs/27k1pdlv\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish/runs/27k1pdlv</a><br/>\n",
       "                Run data is saved locally in <code>/workspace/wandb/run-20210323_234146-27k1pdlv</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2200' max='2200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2200/2200 4:12:51, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>14.934700</td>\n",
       "      <td>19.123951</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.064500</td>\n",
       "      <td>10.107000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>13.427200</td>\n",
       "      <td>17.636900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.771000</td>\n",
       "      <td>10.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>11.927200</td>\n",
       "      <td>14.545224</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.133500</td>\n",
       "      <td>10.298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>nan</td>\n",
       "      <td>3.252018</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.169900</td>\n",
       "      <td>10.291000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>nan</td>\n",
       "      <td>3.017617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.463500</td>\n",
       "      <td>10.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>nan</td>\n",
       "      <td>3.005627</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.341800</td>\n",
       "      <td>10.467000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>nan</td>\n",
       "      <td>3.003123</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.771100</td>\n",
       "      <td>10.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>nan</td>\n",
       "      <td>3.038460</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.577000</td>\n",
       "      <td>10.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.921160</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.293700</td>\n",
       "      <td>10.265000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.914690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.542300</td>\n",
       "      <td>10.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.923331</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.918300</td>\n",
       "      <td>10.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.898105</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.987500</td>\n",
       "      <td>10.329000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.896672</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.515500</td>\n",
       "      <td>10.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.965370</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.184400</td>\n",
       "      <td>10.288000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.885855</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.781400</td>\n",
       "      <td>10.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.888038</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.676100</td>\n",
       "      <td>10.186000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.845855</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.696100</td>\n",
       "      <td>10.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.798287</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.085500</td>\n",
       "      <td>10.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.654631</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.927800</td>\n",
       "      <td>10.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.557739</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.567700</td>\n",
       "      <td>10.208000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.970847</td>\n",
       "      <td>0.926858</td>\n",
       "      <td>49.360700</td>\n",
       "      <td>10.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.502332</td>\n",
       "      <td>0.917496</td>\n",
       "      <td>49.323900</td>\n",
       "      <td>10.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.284557</td>\n",
       "      <td>0.912522</td>\n",
       "      <td>48.975100</td>\n",
       "      <td>10.332000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.270296</td>\n",
       "      <td>0.866296</td>\n",
       "      <td>49.317300</td>\n",
       "      <td>10.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.125605</td>\n",
       "      <td>0.802224</td>\n",
       "      <td>49.152200</td>\n",
       "      <td>10.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.150389</td>\n",
       "      <td>0.766530</td>\n",
       "      <td>49.937100</td>\n",
       "      <td>10.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.146387</td>\n",
       "      <td>0.772967</td>\n",
       "      <td>49.281100</td>\n",
       "      <td>10.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.106834</td>\n",
       "      <td>0.726448</td>\n",
       "      <td>49.954300</td>\n",
       "      <td>10.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.130761</td>\n",
       "      <td>0.703920</td>\n",
       "      <td>49.584200</td>\n",
       "      <td>10.205000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.161090</td>\n",
       "      <td>0.729959</td>\n",
       "      <td>49.232800</td>\n",
       "      <td>10.278000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.150671</td>\n",
       "      <td>0.706554</td>\n",
       "      <td>50.155600</td>\n",
       "      <td>10.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.189870</td>\n",
       "      <td>0.704506</td>\n",
       "      <td>49.279200</td>\n",
       "      <td>10.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.192847</td>\n",
       "      <td>0.686074</td>\n",
       "      <td>49.711100</td>\n",
       "      <td>10.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.202163</td>\n",
       "      <td>0.679345</td>\n",
       "      <td>50.058900</td>\n",
       "      <td>10.108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.209365</td>\n",
       "      <td>0.667934</td>\n",
       "      <td>49.858400</td>\n",
       "      <td>10.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.220738</td>\n",
       "      <td>0.659743</td>\n",
       "      <td>49.808600</td>\n",
       "      <td>10.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.217274</td>\n",
       "      <td>0.662376</td>\n",
       "      <td>50.032200</td>\n",
       "      <td>10.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.247163</td>\n",
       "      <td>0.663546</td>\n",
       "      <td>49.643900</td>\n",
       "      <td>10.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.280349</td>\n",
       "      <td>0.645407</td>\n",
       "      <td>49.852000</td>\n",
       "      <td>10.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.252065</td>\n",
       "      <td>0.645992</td>\n",
       "      <td>50.768600</td>\n",
       "      <td>9.967000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.232286</td>\n",
       "      <td>0.638385</td>\n",
       "      <td>50.377500</td>\n",
       "      <td>10.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.272168</td>\n",
       "      <td>0.628438</td>\n",
       "      <td>49.930300</td>\n",
       "      <td>10.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.306440</td>\n",
       "      <td>0.650380</td>\n",
       "      <td>50.165300</td>\n",
       "      <td>10.087000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.264963</td>\n",
       "      <td>0.641018</td>\n",
       "      <td>50.127600</td>\n",
       "      <td>10.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.314487</td>\n",
       "      <td>0.635167</td>\n",
       "      <td>50.443800</td>\n",
       "      <td>10.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.313774</td>\n",
       "      <td>0.627267</td>\n",
       "      <td>50.534300</td>\n",
       "      <td>10.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.345684</td>\n",
       "      <td>0.624927</td>\n",
       "      <td>50.024400</td>\n",
       "      <td>10.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.394785</td>\n",
       "      <td>0.629023</td>\n",
       "      <td>49.936300</td>\n",
       "      <td>10.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.384007</td>\n",
       "      <td>0.632826</td>\n",
       "      <td>50.293900</td>\n",
       "      <td>10.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.355128</td>\n",
       "      <td>0.632534</td>\n",
       "      <td>50.636100</td>\n",
       "      <td>9.993000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.400469</td>\n",
       "      <td>0.635167</td>\n",
       "      <td>50.262800</td>\n",
       "      <td>10.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.359769</td>\n",
       "      <td>0.626682</td>\n",
       "      <td>50.664200</td>\n",
       "      <td>9.987000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.387897</td>\n",
       "      <td>0.622879</td>\n",
       "      <td>50.177100</td>\n",
       "      <td>10.084000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.411331</td>\n",
       "      <td>0.633704</td>\n",
       "      <td>50.409100</td>\n",
       "      <td>10.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.421035</td>\n",
       "      <td>0.623757</td>\n",
       "      <td>50.513700</td>\n",
       "      <td>10.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.415772</td>\n",
       "      <td>0.624049</td>\n",
       "      <td>50.633900</td>\n",
       "      <td>9.993000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.405144</td>\n",
       "      <td>0.633119</td>\n",
       "      <td>50.576400</td>\n",
       "      <td>10.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.381918</td>\n",
       "      <td>0.619661</td>\n",
       "      <td>50.210200</td>\n",
       "      <td>10.078000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.393476</td>\n",
       "      <td>0.622586</td>\n",
       "      <td>50.321100</td>\n",
       "      <td>10.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.385814</td>\n",
       "      <td>0.623171</td>\n",
       "      <td>50.676700</td>\n",
       "      <td>9.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.449963</td>\n",
       "      <td>0.629608</td>\n",
       "      <td>50.282000</td>\n",
       "      <td>10.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.433227</td>\n",
       "      <td>0.614102</td>\n",
       "      <td>50.862900</td>\n",
       "      <td>9.948000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.437797</td>\n",
       "      <td>0.615272</td>\n",
       "      <td>51.080300</td>\n",
       "      <td>9.906000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.446191</td>\n",
       "      <td>0.619368</td>\n",
       "      <td>50.570100</td>\n",
       "      <td>10.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.420362</td>\n",
       "      <td>0.617905</td>\n",
       "      <td>50.599900</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.439774</td>\n",
       "      <td>0.618198</td>\n",
       "      <td>50.350400</td>\n",
       "      <td>10.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.453789</td>\n",
       "      <td>0.620831</td>\n",
       "      <td>50.291600</td>\n",
       "      <td>10.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.473083</td>\n",
       "      <td>0.609128</td>\n",
       "      <td>50.134900</td>\n",
       "      <td>10.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.423189</td>\n",
       "      <td>0.608250</td>\n",
       "      <td>50.141900</td>\n",
       "      <td>10.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.466859</td>\n",
       "      <td>0.611176</td>\n",
       "      <td>50.092500</td>\n",
       "      <td>10.101000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.477477</td>\n",
       "      <td>0.610006</td>\n",
       "      <td>50.472700</td>\n",
       "      <td>10.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.464355</td>\n",
       "      <td>0.603277</td>\n",
       "      <td>50.446100</td>\n",
       "      <td>10.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.456688</td>\n",
       "      <td>0.598303</td>\n",
       "      <td>50.133100</td>\n",
       "      <td>10.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.496766</td>\n",
       "      <td>0.617905</td>\n",
       "      <td>50.968500</td>\n",
       "      <td>9.928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.465241</td>\n",
       "      <td>0.605617</td>\n",
       "      <td>50.300800</td>\n",
       "      <td>10.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.464164</td>\n",
       "      <td>0.603277</td>\n",
       "      <td>49.947800</td>\n",
       "      <td>10.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.446525</td>\n",
       "      <td>0.605617</td>\n",
       "      <td>50.348600</td>\n",
       "      <td>10.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.451450</td>\n",
       "      <td>0.600644</td>\n",
       "      <td>50.305000</td>\n",
       "      <td>10.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.434475</td>\n",
       "      <td>0.598303</td>\n",
       "      <td>50.231700</td>\n",
       "      <td>10.073000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.443931</td>\n",
       "      <td>0.600644</td>\n",
       "      <td>50.177500</td>\n",
       "      <td>10.084000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.445329</td>\n",
       "      <td>0.599766</td>\n",
       "      <td>50.321200</td>\n",
       "      <td>10.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.447812</td>\n",
       "      <td>0.601814</td>\n",
       "      <td>54.000100</td>\n",
       "      <td>9.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2075</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.458424</td>\n",
       "      <td>0.604740</td>\n",
       "      <td>53.482800</td>\n",
       "      <td>9.461000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.465299</td>\n",
       "      <td>0.603277</td>\n",
       "      <td>53.621700</td>\n",
       "      <td>9.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2125</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.461241</td>\n",
       "      <td>0.601521</td>\n",
       "      <td>53.162200</td>\n",
       "      <td>9.518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.458028</td>\n",
       "      <td>0.600936</td>\n",
       "      <td>50.885200</td>\n",
       "      <td>9.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2175</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.459285</td>\n",
       "      <td>0.602106</td>\n",
       "      <td>50.507700</td>\n",
       "      <td>10.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.460545</td>\n",
       "      <td>0.601814</td>\n",
       "      <td>50.528100</td>\n",
       "      <td>10.014000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10393<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 1203.67MB of 1203.67MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/workspace/wandb/run-20210323_234146-27k1pdlv/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/workspace/wandb/run-20210323_234146-27k1pdlv/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>nan</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/epoch</td><td>100.0</td></tr><tr><td>_runtime</td><td>15175</td></tr><tr><td>_timestamp</td><td>1616558081</td></tr><tr><td>_step</td><td>2200</td></tr><tr><td>eval/loss</td><td>1.46055</td></tr><tr><td>eval/wer</td><td>0.60181</td></tr><tr><td>eval/runtime</td><td>50.5281</td></tr><tr><td>eval/samples_per_second</td><td>10.014</td></tr><tr><td>train/train_runtime</td><td>15174.9386</td></tr><tr><td>train/train_samples_per_second</td><td>0.145</td></tr><tr><td>train/total_flos</td><td>1.4600456240205404e+19</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▁                                      </td></tr><tr><td>train/learning_rate</td><td>▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>█▆▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/wer</td><td>█████████▇▆▅▄▃▃▂▂▂▂▂▂▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▃▂▂▁▂▁▂▂▁▂▁▂▂▂▂▃▃▂▃▃▃▃▄▄▃▄▃▃▄▄▃▃▃▄▃▃▃██▄</td></tr><tr><td>eval/samples_per_second</td><td>▆▇▇█▇█▇▇▇▇█▇▇▇▇▆▆▇▅▆▅▆▅▅▅▅▆▆▄▅▆▆▅▅▆▆▆▁▁▅</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">ie3e-4-200w</strong>: <a href=\"https://wandb.ai/wandb/xlsr-irish/runs/27k1pdlv\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish/runs/27k1pdlv</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100e - LR SCHEDULE 10-40-50, LoudNorm, 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lr_lambda(current_step):\n",
    "#     if current_step < warmup_steps:\n",
    "#         return float(current_step) / float(max(1, warmup_steps))\n",
    "#     elif current_step < warmup_steps+constant_steps:\n",
    "#         return 1\n",
    "#     else: max(\n",
    "#         0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - (warmup_steps+constant_steps)))\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Optimizer, AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from transformers.trainer_utils import SchedulerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flat_linear_schedule_with_warmup(optimizer:Optimizer, num_warmup_steps:int,\n",
    "                                         num_training_steps:int, last_epoch:int =-1):\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        constant_steps = int(num_training_steps * 0.4)\n",
    "        warmup_steps = int(num_training_steps * 0.1)\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        elif current_step < warmup_steps+constant_steps:\n",
    "            return 1\n",
    "        else: return max(\n",
    "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - (warmup_steps+constant_steps)))\n",
    "        )\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "\n",
    "def get_flat_cheduler(\n",
    "    name: Union[str, SchedulerType] = None,\n",
    "    optimizer: Optimizer = None,\n",
    "    num_warmup_steps: Optional[int] = None,\n",
    "    num_training_steps: Optional[int] = None,\n",
    "):\n",
    "    return get_flat_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, \n",
    "                                                num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "     \n",
    "    def create_flat_scheduler(self, num_training_steps: int):\n",
    "        self.lr_scheduler = get_flat_cheduler(optimizer = self.optimizer,\n",
    "                                              num_training_steps=num_training_steps)\n",
    "    \n",
    "    def create_optimizer_and_scheduler(self, num_training_steps):\n",
    "        self.create_optimizer()\n",
    "        self.create_flat_scheduler(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-xlsr-53\", \n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.1,\n",
    "    gradient_checkpointing=True, \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    ctc_zero_infinity = True,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=\"data\",\n",
    "  # output_dir=\"./wav2vec2-large-xlsr-turkish-demo\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=48,\n",
    "  per_device_eval_batch_size=64,\n",
    "  gradient_accumulation_steps=1,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=100,\n",
    "  fp16=True,\n",
    "  save_steps=25,\n",
    "  eval_steps=25,\n",
    "  logging_steps=5,\n",
    "  learning_rate=3e-4,\n",
    "  warmup_steps=200,\n",
    "  save_total_limit=1,\n",
    "    \n",
    "  # WANDB LOGGING: \n",
    "  report_to = 'wandb',  # enable logging to W&B\n",
    "  run_name = 'flat_linear_100e_lr-3e4',   # Name your run, optional\n",
    "  load_best_model_at_end = True,  # This will ensure your best model will be uploaded to W&B\n",
    "  metric_for_best_model='wer',    # Load best model based on \"wer\", not eval loss\n",
    "  greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = FlatTrainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=common_voice_train,\n",
    "    eval_dataset=common_voice_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">flat_linear_100e_lr-3e4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/wandb/xlsr-irish\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/wandb/xlsr-irish/runs/31d3i6ap\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish/runs/31d3i6ap</a><br/>\n",
       "                Run data is saved locally in <code>/workspace/wandb/run-20210324_035553-31d3i6ap</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2200' max='2200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2200/2200 4:12:35, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>14.933500</td>\n",
       "      <td>19.143225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.896900</td>\n",
       "      <td>10.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>13.663600</td>\n",
       "      <td>17.827017</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.479200</td>\n",
       "      <td>10.437000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>12.668800</td>\n",
       "      <td>15.276717</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.623500</td>\n",
       "      <td>10.406000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>nan</td>\n",
       "      <td>3.410819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.637000</td>\n",
       "      <td>10.404000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>nan</td>\n",
       "      <td>3.027001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.633900</td>\n",
       "      <td>10.404000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>nan</td>\n",
       "      <td>3.013434</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.550100</td>\n",
       "      <td>10.422000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.989089</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.651200</td>\n",
       "      <td>10.401000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.990013</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.597800</td>\n",
       "      <td>10.202000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.994911</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.709200</td>\n",
       "      <td>9.785000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.988873</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.441000</td>\n",
       "      <td>9.837000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>nan</td>\n",
       "      <td>3.000114</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.667500</td>\n",
       "      <td>9.793000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.933487</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52.810500</td>\n",
       "      <td>9.581000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.910854</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.378200</td>\n",
       "      <td>10.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.934108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.567100</td>\n",
       "      <td>10.419000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.916187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.755900</td>\n",
       "      <td>10.378000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.899728</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.868600</td>\n",
       "      <td>10.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.890978</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.686800</td>\n",
       "      <td>10.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.887099</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.124800</td>\n",
       "      <td>10.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.908501</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.930000</td>\n",
       "      <td>10.341000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.781373</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.935800</td>\n",
       "      <td>10.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.574728</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.107500</td>\n",
       "      <td>10.304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.134930</td>\n",
       "      <td>0.958455</td>\n",
       "      <td>52.099700</td>\n",
       "      <td>9.712000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.619278</td>\n",
       "      <td>0.973084</td>\n",
       "      <td>53.078800</td>\n",
       "      <td>9.533000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.341110</td>\n",
       "      <td>0.909889</td>\n",
       "      <td>54.089400</td>\n",
       "      <td>9.355000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.209155</td>\n",
       "      <td>0.863078</td>\n",
       "      <td>52.016700</td>\n",
       "      <td>9.728000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.159929</td>\n",
       "      <td>0.809245</td>\n",
       "      <td>50.426100</td>\n",
       "      <td>10.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.114788</td>\n",
       "      <td>0.785840</td>\n",
       "      <td>49.260800</td>\n",
       "      <td>10.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.097395</td>\n",
       "      <td>0.722645</td>\n",
       "      <td>49.177400</td>\n",
       "      <td>10.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.127487</td>\n",
       "      <td>0.720304</td>\n",
       "      <td>49.230500</td>\n",
       "      <td>10.278000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.149613</td>\n",
       "      <td>0.727911</td>\n",
       "      <td>49.272800</td>\n",
       "      <td>10.269000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.206171</td>\n",
       "      <td>0.704506</td>\n",
       "      <td>49.263100</td>\n",
       "      <td>10.271000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.213029</td>\n",
       "      <td>0.703628</td>\n",
       "      <td>49.230700</td>\n",
       "      <td>10.278000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.240126</td>\n",
       "      <td>0.699239</td>\n",
       "      <td>49.730800</td>\n",
       "      <td>10.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.198807</td>\n",
       "      <td>0.691633</td>\n",
       "      <td>49.471500</td>\n",
       "      <td>10.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.286543</td>\n",
       "      <td>0.671153</td>\n",
       "      <td>50.109100</td>\n",
       "      <td>10.098000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.233399</td>\n",
       "      <td>0.665301</td>\n",
       "      <td>49.723300</td>\n",
       "      <td>10.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.249915</td>\n",
       "      <td>0.661498</td>\n",
       "      <td>49.611400</td>\n",
       "      <td>10.199000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.306660</td>\n",
       "      <td>0.681100</td>\n",
       "      <td>49.622900</td>\n",
       "      <td>10.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.351117</td>\n",
       "      <td>0.680807</td>\n",
       "      <td>49.624700</td>\n",
       "      <td>10.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.234659</td>\n",
       "      <td>0.664131</td>\n",
       "      <td>50.035600</td>\n",
       "      <td>10.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.332756</td>\n",
       "      <td>0.665301</td>\n",
       "      <td>49.610100</td>\n",
       "      <td>10.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.341780</td>\n",
       "      <td>0.647162</td>\n",
       "      <td>49.883600</td>\n",
       "      <td>10.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.424674</td>\n",
       "      <td>0.652721</td>\n",
       "      <td>49.792100</td>\n",
       "      <td>10.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.399952</td>\n",
       "      <td>0.650088</td>\n",
       "      <td>49.757400</td>\n",
       "      <td>10.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.405743</td>\n",
       "      <td>0.660913</td>\n",
       "      <td>49.711700</td>\n",
       "      <td>10.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.395725</td>\n",
       "      <td>0.651843</td>\n",
       "      <td>49.852800</td>\n",
       "      <td>10.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.403030</td>\n",
       "      <td>0.662668</td>\n",
       "      <td>49.879500</td>\n",
       "      <td>10.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.359108</td>\n",
       "      <td>0.650380</td>\n",
       "      <td>49.691700</td>\n",
       "      <td>10.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.475290</td>\n",
       "      <td>0.648625</td>\n",
       "      <td>50.030000</td>\n",
       "      <td>10.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.342153</td>\n",
       "      <td>0.618490</td>\n",
       "      <td>49.876200</td>\n",
       "      <td>10.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.488660</td>\n",
       "      <td>0.643359</td>\n",
       "      <td>49.953100</td>\n",
       "      <td>10.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.369161</td>\n",
       "      <td>0.636630</td>\n",
       "      <td>49.968400</td>\n",
       "      <td>10.126000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.433652</td>\n",
       "      <td>0.629608</td>\n",
       "      <td>49.822400</td>\n",
       "      <td>10.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.466875</td>\n",
       "      <td>0.627267</td>\n",
       "      <td>49.923600</td>\n",
       "      <td>10.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.500584</td>\n",
       "      <td>0.626390</td>\n",
       "      <td>49.877500</td>\n",
       "      <td>10.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.462618</td>\n",
       "      <td>0.626390</td>\n",
       "      <td>50.002800</td>\n",
       "      <td>10.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.468548</td>\n",
       "      <td>0.630193</td>\n",
       "      <td>50.105300</td>\n",
       "      <td>10.099000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.420298</td>\n",
       "      <td>0.624634</td>\n",
       "      <td>50.668600</td>\n",
       "      <td>9.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.498389</td>\n",
       "      <td>0.626975</td>\n",
       "      <td>50.223900</td>\n",
       "      <td>10.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.473889</td>\n",
       "      <td>0.637507</td>\n",
       "      <td>50.133300</td>\n",
       "      <td>10.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.502180</td>\n",
       "      <td>0.610298</td>\n",
       "      <td>50.184100</td>\n",
       "      <td>10.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.563186</td>\n",
       "      <td>0.621416</td>\n",
       "      <td>50.043600</td>\n",
       "      <td>10.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.481493</td>\n",
       "      <td>0.611469</td>\n",
       "      <td>50.504800</td>\n",
       "      <td>10.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.506049</td>\n",
       "      <td>0.609713</td>\n",
       "      <td>50.119900</td>\n",
       "      <td>10.096000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.515095</td>\n",
       "      <td>0.604740</td>\n",
       "      <td>50.313900</td>\n",
       "      <td>10.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.489361</td>\n",
       "      <td>0.603862</td>\n",
       "      <td>50.152400</td>\n",
       "      <td>10.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.531780</td>\n",
       "      <td>0.613809</td>\n",
       "      <td>50.090500</td>\n",
       "      <td>10.102000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.536353</td>\n",
       "      <td>0.601814</td>\n",
       "      <td>50.078900</td>\n",
       "      <td>10.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.488343</td>\n",
       "      <td>0.602399</td>\n",
       "      <td>50.187100</td>\n",
       "      <td>10.082000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.496913</td>\n",
       "      <td>0.605325</td>\n",
       "      <td>50.641000</td>\n",
       "      <td>9.992000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.485948</td>\n",
       "      <td>0.601229</td>\n",
       "      <td>50.216900</td>\n",
       "      <td>10.076000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.530440</td>\n",
       "      <td>0.615565</td>\n",
       "      <td>50.835000</td>\n",
       "      <td>9.954000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.540978</td>\n",
       "      <td>0.610591</td>\n",
       "      <td>50.591200</td>\n",
       "      <td>10.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.531480</td>\n",
       "      <td>0.621416</td>\n",
       "      <td>50.363200</td>\n",
       "      <td>10.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.522742</td>\n",
       "      <td>0.610591</td>\n",
       "      <td>50.424700</td>\n",
       "      <td>10.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.545707</td>\n",
       "      <td>0.615565</td>\n",
       "      <td>50.412400</td>\n",
       "      <td>10.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.539567</td>\n",
       "      <td>0.611761</td>\n",
       "      <td>50.563700</td>\n",
       "      <td>10.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.573234</td>\n",
       "      <td>0.612346</td>\n",
       "      <td>50.366400</td>\n",
       "      <td>10.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.533534</td>\n",
       "      <td>0.602984</td>\n",
       "      <td>50.480800</td>\n",
       "      <td>10.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.552885</td>\n",
       "      <td>0.607958</td>\n",
       "      <td>50.225500</td>\n",
       "      <td>10.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.563729</td>\n",
       "      <td>0.612054</td>\n",
       "      <td>50.394100</td>\n",
       "      <td>10.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.562805</td>\n",
       "      <td>0.615565</td>\n",
       "      <td>51.221400</td>\n",
       "      <td>9.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2075</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.572846</td>\n",
       "      <td>0.617905</td>\n",
       "      <td>50.574500</td>\n",
       "      <td>10.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.566526</td>\n",
       "      <td>0.612639</td>\n",
       "      <td>50.581500</td>\n",
       "      <td>10.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2125</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.555435</td>\n",
       "      <td>0.609713</td>\n",
       "      <td>51.014200</td>\n",
       "      <td>9.919000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.548784</td>\n",
       "      <td>0.609713</td>\n",
       "      <td>51.422000</td>\n",
       "      <td>9.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2175</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.546864</td>\n",
       "      <td>0.609713</td>\n",
       "      <td>51.210000</td>\n",
       "      <td>9.881000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.547277</td>\n",
       "      <td>0.609713</td>\n",
       "      <td>53.836900</td>\n",
       "      <td>9.399000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10425<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 1203.67MB of 1203.67MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/workspace/wandb/run-20210324_035553-31d3i6ap/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/workspace/wandb/run-20210324_035553-31d3i6ap/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>nan</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/epoch</td><td>100.0</td></tr><tr><td>_runtime</td><td>15157</td></tr><tr><td>_timestamp</td><td>1616573311</td></tr><tr><td>_step</td><td>2200</td></tr><tr><td>eval/loss</td><td>1.54728</td></tr><tr><td>eval/wer</td><td>0.60971</td></tr><tr><td>eval/runtime</td><td>53.8369</td></tr><tr><td>eval/samples_per_second</td><td>9.399</td></tr><tr><td>train/train_runtime</td><td>15157.3467</td></tr><tr><td>train/train_samples_per_second</td><td>0.145</td></tr><tr><td>train/total_flos</td><td>1.4600456240205404e+19</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▁                                      </td></tr><tr><td>train/learning_rate</td><td>▂▃▅▇█████████████████▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>█▆▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/wer</td><td>███████████▆▄▃▃▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▃▁▁▁▅▇▁▁▂▂▇▆▂▂▂▂▃▂▂▃▃▃▃▃▃▃▃▃▄▃▃▄▄▃▃▄▃▄▄█</td></tr><tr><td>eval/samples_per_second</td><td>▆███▄▂██▇▇▂▃▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▅▆▆▅▅▅▅▅▅▅▅▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">flat_linear_100e_lr-3e4</strong>: <a href=\"https://wandb.ai/wandb/xlsr-irish/runs/31d3i6ap\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish/runs/31d3i6ap</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mask_time_prob = 0.15, Loud Norm 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-xlsr-53\", \n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.15,  # CHANGED HERE FROM 0.05 DEFAULT\n",
    "    layerdrop=0.1,\n",
    "    gradient_checkpointing=True, \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    ctc_zero_infinity = True,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=\"data\",\n",
    "  # output_dir=\"./wav2vec2-large-xlsr-turkish-demo\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=48,\n",
    "  per_device_eval_batch_size=64,\n",
    "  gradient_accumulation_steps=1,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=50,\n",
    "  fp16=True,\n",
    "  save_steps=25,\n",
    "  eval_steps=25,\n",
    "  logging_steps=5,\n",
    "  learning_rate=3e-4,\n",
    "  warmup_steps=200,\n",
    "  save_total_limit=1,\n",
    "    \n",
    "  # WANDB LOGGING: \n",
    "  report_to = 'wandb',  # enable logging to W&B\n",
    "  run_name = 'mask-time-0.15_3e-4-200w',   # Name your run, optional\n",
    "  load_best_model_at_end = True,  # This will ensure your best model will be uploaded to W&B\n",
    "  metric_for_best_model='wer',    # Load best model based on \"wer\", not eval loss\n",
    "  greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=common_voice_train,\n",
    "    eval_dataset=common_voice_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">mask-time-0.15_3e-4-200w</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/wandb/xlsr-irish\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/wandb/xlsr-irish/runs/2j2jdl5n\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish/runs/2j2jdl5n</a><br/>\n",
       "                Run data is saved locally in <code>/workspace/wandb/run-20210324_080951-2j2jdl5n</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1100' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1100/1100 2:05:05, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>14.914800</td>\n",
       "      <td>19.102179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>53.734800</td>\n",
       "      <td>9.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>13.812800</td>\n",
       "      <td>17.599880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.591100</td>\n",
       "      <td>10.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>12.190000</td>\n",
       "      <td>14.832185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52.587500</td>\n",
       "      <td>9.622000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.679500</td>\n",
       "      <td>3.227592</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.808500</td>\n",
       "      <td>9.767000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>3.045000</td>\n",
       "      <td>3.019342</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.278100</td>\n",
       "      <td>9.868000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.049200</td>\n",
       "      <td>2.995693</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.399500</td>\n",
       "      <td>10.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>2.992100</td>\n",
       "      <td>2.971146</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.329400</td>\n",
       "      <td>10.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.957300</td>\n",
       "      <td>2.941744</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.653600</td>\n",
       "      <td>10.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>3.113100</td>\n",
       "      <td>3.256397</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.137800</td>\n",
       "      <td>10.298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.949400</td>\n",
       "      <td>2.916080</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.433300</td>\n",
       "      <td>10.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.912100</td>\n",
       "      <td>2.942116</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.446200</td>\n",
       "      <td>10.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.915000</td>\n",
       "      <td>2.901979</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.717700</td>\n",
       "      <td>10.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>2.893200</td>\n",
       "      <td>2.908299</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.595600</td>\n",
       "      <td>10.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.892000</td>\n",
       "      <td>2.901826</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.738800</td>\n",
       "      <td>10.382000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>2.901800</td>\n",
       "      <td>2.918535</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.521200</td>\n",
       "      <td>10.428000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.882700</td>\n",
       "      <td>2.882536</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.759800</td>\n",
       "      <td>10.377000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>2.934900</td>\n",
       "      <td>2.883935</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.710300</td>\n",
       "      <td>10.388000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.871500</td>\n",
       "      <td>2.875229</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.691500</td>\n",
       "      <td>10.392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>2.852739</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.830600</td>\n",
       "      <td>10.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.775900</td>\n",
       "      <td>2.772469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.078500</td>\n",
       "      <td>10.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>2.722000</td>\n",
       "      <td>2.665955</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.821700</td>\n",
       "      <td>10.364000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.573900</td>\n",
       "      <td>2.494851</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.827000</td>\n",
       "      <td>10.363000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>2.306200</td>\n",
       "      <td>2.181270</td>\n",
       "      <td>0.997074</td>\n",
       "      <td>49.440500</td>\n",
       "      <td>10.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.942700</td>\n",
       "      <td>1.805063</td>\n",
       "      <td>0.947923</td>\n",
       "      <td>49.217900</td>\n",
       "      <td>10.281000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>1.558800</td>\n",
       "      <td>1.495177</td>\n",
       "      <td>0.909596</td>\n",
       "      <td>49.370300</td>\n",
       "      <td>10.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.440800</td>\n",
       "      <td>1.388932</td>\n",
       "      <td>0.946167</td>\n",
       "      <td>48.959700</td>\n",
       "      <td>10.335000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>1.114600</td>\n",
       "      <td>1.282211</td>\n",
       "      <td>0.880047</td>\n",
       "      <td>49.412600</td>\n",
       "      <td>10.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.067700</td>\n",
       "      <td>1.166360</td>\n",
       "      <td>0.834114</td>\n",
       "      <td>49.283400</td>\n",
       "      <td>10.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.895100</td>\n",
       "      <td>1.152554</td>\n",
       "      <td>0.826507</td>\n",
       "      <td>49.664300</td>\n",
       "      <td>10.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.772200</td>\n",
       "      <td>1.100401</td>\n",
       "      <td>0.799005</td>\n",
       "      <td>49.497200</td>\n",
       "      <td>10.223000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.632700</td>\n",
       "      <td>1.144588</td>\n",
       "      <td>0.776770</td>\n",
       "      <td>49.558400</td>\n",
       "      <td>10.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.600600</td>\n",
       "      <td>1.116352</td>\n",
       "      <td>0.768286</td>\n",
       "      <td>49.897900</td>\n",
       "      <td>10.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.495100</td>\n",
       "      <td>1.137980</td>\n",
       "      <td>0.768286</td>\n",
       "      <td>49.593600</td>\n",
       "      <td>10.203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.455500</td>\n",
       "      <td>1.119764</td>\n",
       "      <td>0.750731</td>\n",
       "      <td>50.025800</td>\n",
       "      <td>10.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.382600</td>\n",
       "      <td>1.129640</td>\n",
       "      <td>0.741369</td>\n",
       "      <td>49.751300</td>\n",
       "      <td>10.171000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.443900</td>\n",
       "      <td>1.113171</td>\n",
       "      <td>0.715916</td>\n",
       "      <td>49.802600</td>\n",
       "      <td>10.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.576800</td>\n",
       "      <td>1.115703</td>\n",
       "      <td>0.730837</td>\n",
       "      <td>49.910800</td>\n",
       "      <td>10.138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.346100</td>\n",
       "      <td>1.133567</td>\n",
       "      <td>0.715623</td>\n",
       "      <td>49.764500</td>\n",
       "      <td>10.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.403700</td>\n",
       "      <td>1.130469</td>\n",
       "      <td>0.710357</td>\n",
       "      <td>49.902900</td>\n",
       "      <td>10.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.314000</td>\n",
       "      <td>1.140610</td>\n",
       "      <td>0.712112</td>\n",
       "      <td>49.714500</td>\n",
       "      <td>10.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.315600</td>\n",
       "      <td>1.150014</td>\n",
       "      <td>0.711235</td>\n",
       "      <td>50.072700</td>\n",
       "      <td>10.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>1.152593</td>\n",
       "      <td>0.714453</td>\n",
       "      <td>49.747100</td>\n",
       "      <td>10.171000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.397800</td>\n",
       "      <td>1.160500</td>\n",
       "      <td>0.715916</td>\n",
       "      <td>49.822300</td>\n",
       "      <td>10.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.194100</td>\n",
       "      <td>1.157971</td>\n",
       "      <td>0.712990</td>\n",
       "      <td>49.905400</td>\n",
       "      <td>10.139000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10457<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 1203.67MB of 1203.67MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/workspace/wandb/run-20210324_080951-2j2jdl5n/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/workspace/wandb/run-20210324_080951-2j2jdl5n/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.1941</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/epoch</td><td>50.0</td></tr><tr><td>_runtime</td><td>7509</td></tr><tr><td>_timestamp</td><td>1616580900</td></tr><tr><td>_step</td><td>1100</td></tr><tr><td>eval/loss</td><td>1.15797</td></tr><tr><td>eval/wer</td><td>0.71299</td></tr><tr><td>eval/runtime</td><td>49.9054</td></tr><tr><td>eval/samples_per_second</td><td>10.139</td></tr><tr><td>train/train_runtime</td><td>7509.4342</td></tr><tr><td>train/train_samples_per_second</td><td>0.146</td></tr><tr><td>train/total_flos</td><td>7.283200068608901e+18</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▇▇▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▂▃▄▅▆▇███▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>█▇▆▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/wer</td><td>█████████████████████▇▆▇▅▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▄▆▅▅▂▄▁▂▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▂▂▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>eval/samples_per_second</td><td>▁▅▂▃▄▇▅█▇████████▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">mask-time-0.15_3e-4-200w</strong>: <a href=\"https://wandb.ai/wandb/xlsr-irish/runs/2j2jdl5n\" target=\"_blank\">https://wandb.ai/wandb/xlsr-irish/runs/2j2jdl5n</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADD ENGLISH DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-xlsr-53\", \n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.1,\n",
    "    gradient_checkpointing=True, \n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    ctc_zero_infinity = True,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=\"data\",\n",
    "  # output_dir=\"./wav2vec2-large-xlsr-turkish-demo\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=48,\n",
    "  per_device_eval_batch_size=64,\n",
    "  gradient_accumulation_steps=1,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=20,\n",
    "  fp16=True,\n",
    "  save_steps=25,\n",
    "  eval_steps=25,\n",
    "  logging_steps=5,\n",
    "  learning_rate=3e-4,\n",
    "  warmup_steps=200,\n",
    "  save_total_limit=1,\n",
    "    \n",
    "  # WANDB LOGGING: \n",
    "  report_to = 'wandb',  # enable logging to W&B\n",
    "  run_name = 'ie-en-20e-200w_no_loud_norm',   # Name your run, optional\n",
    "  load_best_model_at_end = True,  # This will ensure your best model will be uploaded to W&B\n",
    "  metric_for_best_model='wer',    # Load best model based on \"wer\", not eval loss\n",
    "  greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=common_voice_train,\n",
    "    eval_dataset=common_voice_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\"patrickvonplaten/wav2vec2-large-xlsr-turkish-demo\").to(\"cuda\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"patrickvonplaten/wav2vec2-large-xlsr-turkish-demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will just take the first example of the test set, run it through the model and take the `argmax(...)` of the logits to retrieve the predicted token ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function.Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "input_dict = processor(common_voice_test[\"input_values\"][0], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "logits = model(input_dict.input_values.to(\"cuda\")).logits\n",
    "\n",
    "pred_ids = torch.argmax(logits, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We adapted `common_voice_test` quite a bit so that the dataset instance does not contain the original sentence label anymore. Thus, we re-use the original dataset to get the label of the first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tr-ad9f7b76efa9f3a0\n",
      "Reusing dataset common_voice (/root/.cache/huggingface/datasets/common_voice/tr-ad9f7b76efa9f3a0/6.1.0/32954a9015faa0d840f6c6894938545c5d12bc5d8936a80079af74bf50d71564)\n"
     ]
    }
   ],
   "source": [
    "common_voice_test_transcription = load_dataset(\"common_voice\", \"tr\", data_dir=\"./cv-corpus-6.1-2020-12-11\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can decode the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "hata küçük şeyler için birbüy bi şeyler kolaluyor ve yenekiçük şeyler için bir bimizi inciltiyoruz\n",
      "\n",
      "Reference:\n",
      "hayatta küçük şeyleri kovalıyor ve yine küçük şeyler için birbirimizi incitiyoruz.\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction:\")\n",
    "print(processor.decode(pred_ids))\n",
    "\n",
    "print(\"\\nReference:\")\n",
    "print(common_voice_test_transcription[\"sentence\"][0].lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! The transcription can definitely be recognized from our prediction, but it is far from being perfect. Training the model a bit longer, spending more time on the data preprocessing, and especially using a language model for decoding would certainly improve the model's overall performance. \n",
    "\n",
    "For a demonstration model on a low-resource language, the results are acceptable, however 🤗."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
