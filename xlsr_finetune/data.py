# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/02_aug.ipynb (unless otherwise specified).

__all__ = ['file_exists', 'ds_file_exists', 'filter_for_exists', 'drop_missing_files', 'add_ds',
           'remove_special_characters', 'chars_to_ignore_regex', 'extract_all_chars', 'get_char_vocab', 'process_vocab',
           'extract_vocab', 'speech_file_to_array', 'file_exists', 'ds_file_exists', 'filter_for_exists',
           'drop_missing_files', 'add_ds', 'remove_special_characters', 'chars_to_ignore_regex', 'extract_all_chars',
           'get_char_vocab', 'process_vocab', 'extract_vocab', 'speech_file_to_array_fn']

# Cell
import os
import re
import json
import numpy as np
from pathlib import Path
from functools import partial

# Cell
import librosa
import torchaudio
from datasets import load_dataset, load_metric, concatenate_datasets

# Cell
def file_exists(e):
    e['file_exists'] = os.path.isfile(e['path'])
    return e

# Cell
def ds_file_exists(ds):
    l_ds = len(ds)
    ds = ds.map(file_exists)
    l_not_found = l_ds-sum(ds['file_exists'])
    if l_not_found == 0: print('All files found')
    else: print(f"{l_not_found} ({(l_not_found/l_ds)*100}%) files not found")
    return ds

# Cell
def filter_for_exists(ds, drop_exist_col=True):
    # Filter dataset to only files that exist
    ds = ds.filter(lambda example: example['file_exists'])

    if drop_exist_col:
        # drop file_exists column
        ds = ds.remove_columns('file_exists')
    return ds

# Cell
def drop_missing_files(ds, drop_exist_col=True):
    ds = ds_file_exists(ds)
    ds = filter_for_exists(ds, drop_exist_col)
    return ds

# Cell
def add_ds(e, new_ds):
    for f in e.keys():
        e[f] = e[f] + new_ds[f]
    return e

# Cell

chars_to_ignore_regex = '[\,\?\.\!\-\;\:\"\“\%\‘\”\�\(\)\-\*]'

def remove_special_characters(batch):
    batch["sentence"] = re.sub(chars_to_ignore_regex, '', batch["sentence"]).lower() + " "
    batch["sentence"] = re.sub('[\’]', '\'', batch["sentence"])
    batch["sentence"] = re.sub('[\’]', '\'', batch["sentence"])
    batch["sentence"] = re.sub('[\–]', '-', batch["sentence"])
    batch["sentence"] = re.sub('[\—]', '-', batch["sentence"])
    batch["sentence"] = re.sub('[&]', ' and ', batch["sentence"])
    return batch

# Cell
def extract_all_chars(batch):
    '''merge all texts into one and create set'''
    all_text = " ".join(batch["sentence"])
    vocab = list(set(all_text))
    return {"vocab": [vocab], "all_text": [all_text]}

# Cell
def get_char_vocab(train_ds, test_ds=None):
    train_vocab = ds.map(extract_all_chars, batched=True, batch_size=-1,
                   keep_in_memory=True, remove_columns=ds.column_names)
    if test_ds is not None:
        test_vocab = test_ds.map(extract_all_chars, batched=True, batch_size=-1,
                   keep_in_memory=True, remove_columns=ds.column_names)
        vocab_list = list(set(train_vocab["vocab"][0]) | set(test_ds["vocab"][0]))
    else:
        vocab_list = list(set(train_vocab["vocab"][0]))

    vocab_dict = {v: k for k, v in enumerate(vocab_list)}
    return vocab_dict

# Cell
def process_vocab(vocab_dict):
    vocab_dict["|"] = vocab_dict[" "]
    del vocab_dict[" "]

    vocab_dict["[UNK]"] = len(vocab_dict)
    vocab_dict["[PAD]"] = len(vocab_dict)
    return vocab_dict

# Cell
def extract_vocab(ds, save=True, save_dir='data', fn='vocab.json'):
    vocab = get_char_vocab(ds)
    vocab = process_vocab(vocab)
    if save:
        Path(f"{save_dir}").mkdir(parents=True, exist_ok=True)
        with open(f'{path}/{fn}', 'w') as vocab_file:
            json.dump(vocab, vocab_file)
    return vocab

# Cell
def speech_file_to_array(batch, resample=True, new_sr=16_000):
    try:
        speech_array, sampling_rate = torchaudio.load(batch["path"])

        if resample:
            batch["speech"] = librosa.resample(np.asarray(speech_array[0].numpy()), sampling_rate, new_sr)
            batch["sampling_rate"] = new_sr
        else:
            batch["speech"] = speech_array[0].numpy()
            batch["sampling_rate"] = sampling_rate
    except:
        batch["speech"] = np.array([0])
        batch["sampling_rate"] = 0
    return batch

# Cell
import os
import re
import json
from pathlib import Path
from functools import partial

# Cell
from datasets import load_dataset, load_metric, concatenate_datasets
import torchaudio

# Cell
def file_exists(e):
    e['file_exists'] = os.path.isfile(e['path'])
    return e

# Cell
def ds_file_exists(ds):
    l_ds = len(ds)
    ds = ds.map(file_exists)
    l_not_found = l_ds-sum(ds['file_exists'])
    if l_not_found == 0: print('All files found')
    else: print(f"{l_not_found} ({(l_not_found/l_ds)*100}%) files not found")
    return ds

# Cell
def filter_for_exists(ds, drop_exist_col=True):
    # Filter dataset to only files that exist
    ds = ds.filter(lambda example: example['file_exists'])

    if drop_exist_col:
        # drop file_exists column
        ds = ds.remove_columns('file_exists')
    return ds

# Cell
def drop_missing_files(ds, drop_exist_col=True):
    ds = ds_file_exists(ds)
    ds = filter_for_exists(ds, drop_exist_col)
    return ds

# Cell
def add_ds(e, new_ds):
    for f in e.keys():
        e[f] = e[f] + new_ds[f]
    return e

# Cell

chars_to_ignore_regex = '[\,\?\.\!\-\;\:\"\“\%\‘\”\�\(\)\-\*]'

def remove_special_characters(batch):
    batch["sentence"] = re.sub(chars_to_ignore_regex, '', batch["sentence"]).lower() + " "
    batch["sentence"] = re.sub('[\’]', '\'', batch["sentence"])
    batch["sentence"] = re.sub('[\’]', '\'', batch["sentence"])
    batch["sentence"] = re.sub('[\–]', '-', batch["sentence"])
    batch["sentence"] = re.sub('[\—]', '-', batch["sentence"])
    batch["sentence"] = re.sub('[&]', ' and ', batch["sentence"])
    return batch

# Cell
def extract_all_chars(batch):
    '''merge all texts into one and create set'''
    all_text = " ".join(batch["sentence"])
    vocab = list(set(all_text))
    return {"vocab": [vocab], "all_text": [all_text]}

# Cell
def get_char_vocab(train_ds, test_ds=None):
    train_vocab = ds.map(extract_all_chars, batched=True, batch_size=-1,
                   keep_in_memory=True, remove_columns=ds.column_names)
    if test_ds is not None:
        test_vocab = test_ds.map(extract_all_chars, batched=True, batch_size=-1,
                   keep_in_memory=True, remove_columns=ds.column_names)
        vocab_list = list(set(train_vocab["vocab"][0]) | set(test_ds["vocab"][0]))
    else:
        vocab_list = list(set(train_vocab["vocab"][0]))

    vocab_dict = {v: k for k, v in enumerate(vocab_list)}
    return vocab_dict

# Cell
def process_vocab(vocab_dict):
    vocab_dict["|"] = vocab_dict[" "]
    del vocab_dict[" "]

    vocab_dict["[UNK]"] = len(vocab_dict)
    vocab_dict["[PAD]"] = len(vocab_dict)
    return vocab_dict

# Cell
def extract_vocab(ds, save=True, save_dir='data', fn='vocab.json'):
    vocab = get_char_vocab(ds)
    vocab = process_vocab(vocab)
    if save:
        Path(f"{save_dir}").mkdir(parents=True, exist_ok=True)
        with open(f'{path}/{fn}', 'w') as vocab_file:
            json.dump(vocab, vocab_file)
    return vocab

# Cell
def speech_file_to_array_fn(batch):
    try:
        speech_array, sampling_rate = torchaudio.load(batch["path"])
        batch["speech"] = speech_array[0].numpy()
        batch["sampling_rate"] = sampling_rate
    except:
        batch["speech"] = np.array([0])
        batch["sampling_rate"] = 0

    batch["target_text"] = batch["sentence"]
    return batch